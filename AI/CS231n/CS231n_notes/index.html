<!DOCTYPE html><html class=no-js lang=en> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="zl 的个人网站" name=description><meta content=zl name=author><link href=https://zlflly.github.io/notes/AI/CS231n/CS231n_notes/ rel=canonical><link href=../../%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/ rel=prev><link href=../Image%20Classification-Data-driven%20Approach%2C%20k-Nearest%20Neighbor%2C%20train_val_test%20splits/ rel=next><link href=../../../assets/images/avatar.png rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.6.4" name=generator><title>Computer Vision - zl 的咖啡馆</title><link href=../../../assets/stylesheets/main.8608ea7d.min.css rel=stylesheet><link href=../../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link href=../../../css/toc_extra.css rel=stylesheet><link href=../../../css/heti.css rel=stylesheet><link href=https://cdn.tonycrane.cc/utils/katex.min.css rel=stylesheet><link href=https://cdn.tonycrane.cc/jbmono/jetbrainsmono.css rel=stylesheet><link href=https://cdn.tonycrane.cc/lxgw/lxgwscreen.css rel=stylesheet><link href=../../../css/custom.min.css rel=stylesheet><link href=../../../css/tasklist.min.css rel=stylesheet><link href=../../../css/card.min.css rel=stylesheet><link href=../../../css/flink.min.css rel=stylesheet><link href=../../../css/extra.min.css rel=stylesheet><link href=../../../css/fold_toc.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css rel=stylesheet><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#computer-vision> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav aria-label=Header class="md-header__inner md-grid"> <a aria-label="zl 的咖啡馆" class="md-header__button md-logo" data-md-component=logo href=../../.. title="zl 的咖啡馆"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> zl 的咖啡馆 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Computer Vision </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="follow system" class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="follow system"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg> </label> <input aria-label="light mode" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary=white data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input aria-label=darkmode class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary=black data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_0 hidden title=darkmode> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label=Search class=md-search__options> <a aria-label=Share class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button aria-label=Clear class="md-search__icon md-icon" tabindex=-1 title=Clear type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix tabindex=0> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/zlflly/notes/ title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg> </div> <div class=md-source__repository> zl's café </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav aria-label=Tabs class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a class=md-tabs__link href=../../..> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"></path></svg> Home </a> </li> <li class=md-tabs__item> <a href=../../../Blogs/ class=md-tabs__link> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"></path></svg> Blogs </a> </li> <li class=md-tabs__item> <a href=../../../Summaries/ class=md-tabs__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"></path></svg> Summaries </a> </li> <li class=md-tabs__item> <a href=../../../CS_Basic/ class=md-tabs__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> Computer Basic </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> AI </a> </li> <li class=md-tabs__item> <a href=../../../Robot/ class=md-tabs__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> Robot </a> </li> <li class=md-tabs__item> <a href=../../../Tools/ class=md-tabs__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16h-2v-1H8v1H6v-1H2v5h20v-5h-4zm2-8h-3V6c0-1.1-.9-2-2-2H9c-1.1 0-2 .9-2 2v2H4c-1.1 0-2 .9-2 2v4h4v-2h2v2h8v-2h2v2h4v-4c0-1.1-.9-2-2-2m-5 0H9V6h6z"></path></svg> Tools </a> </li> <li class=md-tabs__item> <a href=../../../Tags/ class=md-tabs__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M5.5 7A1.5 1.5 0 0 1 4 5.5 1.5 1.5 0 0 1 5.5 4 1.5 1.5 0 0 1 7 5.5 1.5 1.5 0 0 1 5.5 7m15.91 4.58-9-9C12.05 2.22 11.55 2 11 2H4c-1.11 0-2 .89-2 2v7c0 .55.22 1.05.59 1.41l8.99 9c.37.36.87.59 1.42.59s1.05-.23 1.41-.59l7-7c.37-.36.59-.86.59-1.41 0-.56-.23-1.06-.59-1.42"></path></svg> Tags </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=Navigation class="md-nav md-nav--primary md-nav--lifted" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="zl 的咖啡馆" class="md-nav__button md-logo" data-md-component=logo href=../../.. title="zl 的咖啡馆"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg> </a> zl 的咖啡馆 </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/zlflly/notes/ title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg> </div> <div class=md-source__repository> zl's café </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_1 type=checkbox> <div class="md-nav__link md-nav__container"> <a class=md-nav__link href=../../..> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"></path></svg> <span class=md-ellipsis> Home </span> </a> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=false aria-labelledby=__nav_1_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-91.9-186.5C182 346.2 212.6 368 256 368s74-21.8 91.9-42.5c5.8-6.7 15.9-7.4 22.6-1.6s7.4 15.9 1.6 22.6c-22.3 25.6-61 53.5-116.1 53.5s-93.8-27.9-116.1-53.5c-5.8-6.7-5.1-16.8 1.6-22.6s16.8-5.1 22.6 1.6M144.4 208a32 32 0 1 1 64 0 32 32 0 1 1-64 0m156.4 25.6c-5.3 7.1-15.3 8.5-22.4 3.2s-8.5-15.3-3.2-22.4c30.4-40.5 91.2-40.5 121.6 0 5.3 7.1 3.9 17.1-3.2 22.4s-17.1 3.9-22.4-3.2c-17.6-23.5-52.8-23.5-70.4 0"></path></svg> <span class=md-ellipsis> About 🥳 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_2 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../../Blogs/ class=md-nav__link> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"></path></svg> <span class=md-ellipsis> Blogs </span> </a> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=false aria-labelledby=__nav_2_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blogs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Blogs/archives/ class=md-nav__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 3h18v4H3zm1 5h16v13H4zm5.5 3a.5.5 0 0 0-.5.5V13h6v-1.5a.5.5 0 0 0-.5-.5z"></path></svg> <span class=md-ellipsis> Archives </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_2_3 type=checkbox> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> Posts </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_2_3_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Posts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/24-12-29.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/24-12-30.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/25-01-20.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href="../../../Blogs/posts/FreeSplatter 代码解读.md"> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/Gaussian_Splatting_Code.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/Gaussian_Splatting_复现.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/OCRN.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/ULIP-2.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/用AI后的问题.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Blogs/posts/notes_software.md> <span class=md-ellipsis> None </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_3 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../../Summaries/ class=md-nav__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"></path></svg> <span class=md-ellipsis> Summaries </span> </a> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=false aria-labelledby=__nav_3_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Summaries </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_3_2 type=checkbox> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> <span class=md-ellipsis> Weekly </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_3_2_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Weekly </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_3_2_1 type=checkbox> <label class=md-nav__link for=__nav_3_2_1 id=__nav_3_2_1_label tabindex=0> <span class=md-ellipsis> 2024 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_3_2_1_label class=md-nav data-md-level=3> <label class=md-nav__title for=__nav_3_2_1> <span class="md-nav__icon md-icon"></span> 2024 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2024/weekly/2024-W44-10.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2024/weekly/2024-W45-11.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2024/weekly/2024-W46-11.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2024/weekly/2024-W47-11.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2024/weekly/2024-W48-11.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2024/weekly/2024-W49-12.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2024/weekly/2024-W50-12.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a href=../../../Summaries/2024/weekly/2024-W51-12/ class=md-nav__link> <span class=md-ellipsis> 2024-W51-12 </span> </a> </li> <li class=md-nav__item> <a href=../../../Summaries/2024/weekly/2024-W52-12/ class=md-nav__link> <span class=md-ellipsis> 2024-W52-12 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_3_2_2 type=checkbox> <label class=md-nav__link for=__nav_3_2_2 id=__nav_3_2_2_label tabindex=0> <span class=md-ellipsis> 2025 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_3_2_2_label class=md-nav data-md-level=3> <label class=md-nav__title for=__nav_3_2_2> <span class="md-nav__icon md-icon"></span> 2025 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2025/weekly/2025-W01-12.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2025/weekly/2025-W02-01.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2025/weekly/2025-W03-01.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2025/weekly/2025-W04-01.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/2025/weekly/2025-W05-01.md> <span class=md-ellipsis> None </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_3_3 type=checkbox> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex=0> <span class=md-ellipsis> Semesters </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_3_3_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Semesters </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/Semesters/202409-10.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../../Summaries/Semesters/2024_1_1.md> <span class=md-ellipsis> None </span> </a> </li> <li class=md-nav__item> <a href=../../../Summaries/Semesters/2024summer_vacation/ class=md-nav__link> <span class=md-ellipsis> 2024年高三-大一暑假总结 </span> </a> </li> <li class=md-nav__item> <a href=../../../Summaries/Semesters/2025winter_vacation/ class=md-nav__link> <span class=md-ellipsis> 2025年寒假总结 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_4 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../../CS_Basic/ class=md-nav__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> <span class=md-ellipsis> Computer Basic </span> </a> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=false aria-labelledby=__nav_4_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Computer Basic </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../CS_Basic/15-213/CSAPP/ class=md-nav__link> <span class=md-ellipsis> CSAPP </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_4_3 type=checkbox> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> C++ </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_4_3_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> C++ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../CS_Basic/C%2B%2B/Accelerated%20C%2B%2B/ class=md-nav__link> <span class=md-ellipsis> Accelerated C++ </span> </a> </li> <li class=md-nav__item> <a href=../../../CS_Basic/C%2B%2B/C%2B%2B%20Basic/ class=md-nav__link> <span class=md-ellipsis> C++ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_4_4 type=checkbox> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex=0> <span class=md-ellipsis> CS61 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_4_4_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> CS61 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../CS_Basic/CS61A/CS61A/ class=md-nav__link> <span class=md-ellipsis> CS61A </span> </a> </li> <li class=md-nav__item> <a href=../../../CS_Basic/CS61A/Composing_Programs/ class=md-nav__link> <span class=md-ellipsis> COMPOSING PROGRAMS </span> </a> </li> <li class=md-nav__item> <a href=../../../CS_Basic/CS61C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E4%B8%8E%E8%AE%BE%E8%AE%A1%E7%A1%AC%E4%BB%B6%E8%BD%AF%E4%BB%B6%E6%8E%A5%E5%8F%A3/ class=md-nav__link> <span class=md-ellipsis> 计算机组成与设计硬件软件接口 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_4_5 type=checkbox> <label class=md-nav__link for=__nav_4_5 id=__nav_4_5_label tabindex=0> <span class=md-ellipsis> Network </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_4_5_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> Network </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../CS_Basic/Network/Security/ class=md-nav__link> <span class=md-ellipsis> Security </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_5 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../ class=md-nav__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> <span class=md-ellipsis> AI </span> </a> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=true aria-labelledby=__nav_5_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> AI </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_5_2 type=checkbox> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class=md-ellipsis> Basic </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_5_2_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> Basic </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Dive_into_Deep_Learning/ class=md-nav__link> <span class=md-ellipsis> Dive into Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../../SLAM14/ class=md-nav__link> <span class=md-ellipsis> 视觉SLAM十四讲 </span> </a> </li> <li class=md-nav__item> <a href=../../%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/ class=md-nav__link> <span class=md-ellipsis> 统计学习方法 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_5_3 type=checkbox> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class=md-ellipsis> CS231n </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_5_3_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> CS231n </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Computer Vision </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Computer Vision </span> </a> <nav aria-label="Table of contents" class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#1-introduction> <span class=md-ellipsis> 1 - Introduction </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#2-image-classification> <span class=md-ellipsis> 2 - Image Classification </span> </a> <nav aria-label="2 - Image Classification" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#k-nearest-neighbor> <span class=md-ellipsis> K Nearest Neighbor </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#linear-classifier> <span class=md-ellipsis> Linear Classifier </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#3-loss-functions-and-optimization> <span class=md-ellipsis> 3 - Loss Functions and Optimization </span> </a> <nav aria-label="3 - Loss Functions and Optimization" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#loss-functions> <span class=md-ellipsis> Loss Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#optimization> <span class=md-ellipsis> Optimization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#4-neural-networks-and-backpropagation> <span class=md-ellipsis> 4 - Neural Networks and Backpropagation </span> </a> <nav aria-label="4 - Neural Networks and Backpropagation" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#neural-networks> <span class=md-ellipsis> Neural Networks </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#backpropagation> <span class=md-ellipsis> Backpropagation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#5-convolutional-neural-networks> <span class=md-ellipsis> 5 - Convolutional Neural Networks </span> </a> <nav aria-label="5 - Convolutional Neural Networks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#convolution-layer> <span class=md-ellipsis> Convolution Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#pooling-layer> <span class=md-ellipsis> Pooling layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#convolutional-neural-networks-cnn> <span class=md-ellipsis> Convolutional Neural Networks (CNN) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#6-cnn-architectures> <span class=md-ellipsis> 6 - CNN Architectures </span> </a> <nav aria-label="6 - CNN Architectures" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#alexnet> <span class=md-ellipsis> AlexNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#vgg> <span class=md-ellipsis> VGG </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#googlenet> <span class=md-ellipsis> GoogLeNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#resnet> <span class=md-ellipsis> ResNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#senet> <span class=md-ellipsis> SENet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#improvements-of-resnet> <span class=md-ellipsis> Improvements of ResNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#other-interesting-networks> <span class=md-ellipsis> Other Interesting Networks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#7-training-neural-networks> <span class=md-ellipsis> 7 - Training Neural Networks </span> </a> <nav aria-label="7 - Training Neural Networks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#activation-functions> <span class=md-ellipsis> Activation Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-processing> <span class=md-ellipsis> Data Processing </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#weight-initialization> <span class=md-ellipsis> Weight Initialization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#batch-normalization> <span class=md-ellipsis> Batch Normalization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transfer-learning> <span class=md-ellipsis> Transfer Learning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#regularization> <span class=md-ellipsis> Regularization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#hyperparameter-tuning> <span class=md-ellipsis> Hyperparameter Tuning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#gradient-checks> <span class=md-ellipsis> Gradient Checks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#8-visualizing-and-understanding> <span class=md-ellipsis> 8 - Visualizing and Understanding </span> </a> <nav aria-label="8 - Visualizing and Understanding" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#feature-visualization-and-inversion> <span class=md-ellipsis> Feature Visualization and Inversion </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#adversarial-examples> <span class=md-ellipsis> Adversarial Examples </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#deepdream-and-style-transfer> <span class=md-ellipsis> DeepDream and Style Transfer </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#9-object-detection-and-image-segmentation> <span class=md-ellipsis> 9 - Object Detection and Image Segmentation </span> </a> <nav aria-label="9 - Object Detection and Image Segmentation" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#semantic-segmentation> <span class=md-ellipsis> Semantic Segmentation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#object-detection> <span class=md-ellipsis> Object Detection </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#instance-segmentation> <span class=md-ellipsis> Instance Segmentation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#10-recurrent-neural-networks> <span class=md-ellipsis> 10 - Recurrent Neural Networks </span> </a> <nav aria-label="10 - Recurrent Neural Networks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#recurrent-neural-network-rnn> <span class=md-ellipsis> Recurrent Neural Network (RNN) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#long-short-term-memory-lstm> <span class=md-ellipsis> Long Short Term Memory (LSTM) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#other-rnn-variants> <span class=md-ellipsis> Other RNN Variants </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#11-attention-and-transformers> <span class=md-ellipsis> 11 - Attention and Transformers </span> </a> <nav aria-label="11 - Attention and Transformers" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#rnn-with-attention> <span class=md-ellipsis> RNN with Attention </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#general-attention-layer> <span class=md-ellipsis> General Attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#self-attention-layer> <span class=md-ellipsis> Self-attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#positional-encoding> <span class=md-ellipsis> Positional Encoding </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#masked-self-attention-layer> <span class=md-ellipsis> Masked Self-attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#multi-head-self-attention-layer> <span class=md-ellipsis> Multi-head Self-attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transformer> <span class=md-ellipsis> Transformer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#comparing-rnns-to-transformer> <span class=md-ellipsis> Comparing RNNs to Transformer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#comparing-convnets-to-transformer> <span class=md-ellipsis> Comparing ConvNets to Transformer </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#12-video-understanding> <span class=md-ellipsis> 12 - Video Understanding </span> </a> <nav aria-label="12 - Video Understanding" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#video-classification> <span class=md-ellipsis> Video Classification </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#plain-cnn-structure> <span class=md-ellipsis> Plain CNN Structure </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#modeling-long-term-temporal-structure> <span class=md-ellipsis> Modeling Long-term Temporal Structure </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#visualizing-video-models> <span class=md-ellipsis> Visualizing Video Models </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#multimodal-video-understanding> <span class=md-ellipsis> Multimodal Video Understanding </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#13-generative-models> <span class=md-ellipsis> 13 - Generative Models </span> </a> <nav aria-label="13 - Generative Models" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#pixelrnn-and-pixelcnn> <span class=md-ellipsis> PixelRNN and PixelCNN </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#variational-autoencoder> <span class=md-ellipsis> Variational Autoencoder </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#generative-adversarial-networks-gans> <span class=md-ellipsis> Generative Adversarial Networks (GANs) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#14-self-supervised-learning> <span class=md-ellipsis> 14 - Self-supervised Learning </span> </a> <nav aria-label="14 - Self-supervised Learning" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#pretext-tasks> <span class=md-ellipsis> Pretext Tasks </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#contrastive-representation-learning> <span class=md-ellipsis> Contrastive Representation Learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#15-low-level-vision> <span class=md-ellipsis> 15 - Low-Level Vision </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#16-3d-vision> <span class=md-ellipsis> 16 - 3D Vision </span> </a> <nav aria-label="16 - 3D Vision" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#representation> <span class=md-ellipsis> Representation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#ai-3d> <span class=md-ellipsis> AI + 3D </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../Image%20Classification-Data-driven%20Approach%2C%20k-Nearest%20Neighbor%2C%20train_val_test%20splits/ class=md-nav__link> <span class=md-ellipsis> Image Classification-Data-driven Approach, k-Nearest Neighbor, train_val_test splits </span> </a> </li> <li class=md-nav__item> <a href=../Linear%20classification-Support%20Vector%20Machine%2C%20Softmax/ class=md-nav__link> <span class=md-ellipsis> Linear classification-Support Vector Machine, Softmax </span> </a> </li> <li class=md-nav__item> <a href=../Numpy/ class=md-nav__link> <span class=md-ellipsis> Numpy </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_5_4 type=checkbox> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex=0> <span class=md-ellipsis> EECS 498-007 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_5_4_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> EECS 498-007 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../EECS%20498-007/KNN/ class=md-nav__link> <span class=md-ellipsis> KNN </span> </a> </li> <li class=md-nav__item> <a href=../../EECS%20498-007/Pytorch/ class=md-nav__link> <span class=md-ellipsis> pytorch 的基本使用 </span> </a> </li> <li class=md-nav__item> <a href=../../EECS%20498-007/linear_classifer/ class=md-nav__link> <span class=md-ellipsis> Linear classifer </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_5_5 type=checkbox> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex=0> <span class=md-ellipsis> FFB6D </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_5_5_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> FFB6D </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../FFB6D/FFB6D_Conda/ class=md-nav__link> <span class=md-ellipsis> FFB6D环境配置指南：原生系统安装 </span> </a> </li> <li class=md-nav__item> <a href=../../FFB6D/FFB6D_Docker/ class=md-nav__link> <span class=md-ellipsis> Docker从入门到实践：以FFB6D环境配置为例 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_6 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../../Robot/ class=md-nav__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> <span class=md-ellipsis> Robot </span> </a> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=false aria-labelledby=__nav_6_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Robot </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Robot/calibration/ class=md-nav__link> <span class=md-ellipsis> Calibration </span> </a> </li> <li class=md-nav__item> <a href=../../../Robot/kalman/ class=md-nav__link> <span class=md-ellipsis> 卡尔曼滤波 </span> </a> </li> <li class=md-nav__item> <a href=../../../Robot/pnp/ class=md-nav__link> <span class=md-ellipsis> pnp </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../../Tools/ class=md-nav__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16h-2v-1H8v1H6v-1H2v5h20v-5h-4zm2-8h-3V6c0-1.1-.9-2-2-2H9c-1.1 0-2 .9-2 2v2H4c-1.1 0-2 .9-2 2v4h4v-2h2v2h8v-2h2v2h4v-4c0-1.1-.9-2-2-2m-5 0H9V6h6z"></path></svg> <span class=md-ellipsis> Tools </span> </a> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=false aria-labelledby=__nav_7_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Tools </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_2 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../../Tools/Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> Cheat Sheet </span> </a> <label class=md-nav__link for=__nav_7_2 id=__nav_7_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=false aria-labelledby=__nav_7_2_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_7_2> <span class="md-nav__icon md-icon"></span> Cheat Sheet </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_2_2 type=checkbox> <label class=md-nav__link for=__nav_7_2_2 id=__nav_7_2_2_label tabindex=0> <span class=md-ellipsis> tools </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_2_2_label class=md-nav data-md-level=3> <label class=md-nav__title for=__nav_7_2_2> <span class="md-nav__icon md-icon"></span> tools </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/adb%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> adb Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/bash%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> bash Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/ffmpeg%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> ffmpeg Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/gdb%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> gdb Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/git%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> git </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/ip%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> ip Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/tools/tmux%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> tmux Cheat Sheet </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_2_3 type=checkbox> <label class=md-nav__link for=__nav_7_2_3 id=__nav_7_2_3_label tabindex=0> <span class=md-ellipsis> editors </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_2_3_label class=md-nav data-md-level=3> <label class=md-nav__title for=__nav_7_2_3> <span class="md-nav__icon md-icon"></span> editors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/emacs%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> emacs Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/nano%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> nano Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/org%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> org Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/editors/vim%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> vim Cheat Sheet </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_2_4 type=checkbox> <label class=md-nav__link for=__nav_7_2_4 id=__nav_7_2_4_label tabindex=0> <span class=md-ellipsis> languages </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_2_4_label class=md-nav data-md-level=3> <label class=md-nav__title for=__nav_7_2_4> <span class="md-nav__icon md-icon"></span> languages </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/javascript%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> javascript Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/python%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> python Cheat Sheet </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Cheat%20Sheet/languages/vimscript%20Cheat%20Sheet/ class=md-nav__link> <span class=md-ellipsis> VimScript Cheat Sheet </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_3 type=checkbox> <label class=md-nav__link for=__nav_7_3 id=__nav_7_3_label tabindex=0> <span class=md-ellipsis> AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_3_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_7_3> <span class="md-nav__icon md-icon"></span> AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/AI/prompt/ class=md-nav__link> <span class=md-ellipsis> prompt </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/AI/prompt_writing/ class=md-nav__link> <span class=md-ellipsis> AI 使用 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_4 type=checkbox> <label class=md-nav__link for=__nav_7_4 id=__nav_7_4_label tabindex=0> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_4_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_7_4> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Blog/Mkdocs_Material/ class=md-nav__link> <span class=md-ellipsis> mkdocs material 超全配置 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_5 type=checkbox> <label class=md-nav__link for=__nav_7_5 id=__nav_7_5_label tabindex=0> <span class=md-ellipsis> Environment </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_5_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_7_5> <span class="md-nav__icon md-icon"></span> Environment </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Environment/Ubuntu_setup/ class=md-nav__link> <span class=md-ellipsis> Ubuntu 配置 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Environment/environment/ class=md-nav__link> <span class=md-ellipsis> Environment </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Environment/obsidian_setup/ class=md-nav__link> <span class=md-ellipsis> obsidian 配置 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_6 type=checkbox> <label class=md-nav__link for=__nav_7_6 id=__nav_7_6_label tabindex=0> <span class=md-ellipsis> Make </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_6_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_7_6> <span class="md-nav__icon md-icon"></span> Make </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Make/CMake/ class=md-nav__link> <span class=md-ellipsis> CMake 相关 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Make/Makeflie/ class=md-nav__link> <span class=md-ellipsis> Makeflie </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_7 type=checkbox> <label class=md-nav__link for=__nav_7_7 id=__nav_7_7_label tabindex=0> <span class=md-ellipsis> Others </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_7_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_7_7> <span class="md-nav__icon md-icon"></span> Others </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Others/Chezmoi/ class=md-nav__link> <span class=md-ellipsis> 用 chezmoi 实现跨设备同步配置 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Others/SSH/ class=md-nav__link> <span class=md-ellipsis> SSH配置指南 </span> </a> </li> <li class=md-nav__item> <a href=../../../Tools/Others/zotero_%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/ class=md-nav__link> <span class=md-ellipsis> zotero_使用指南 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_7_8 type=checkbox> <label class=md-nav__link for=__nav_7_8 id=__nav_7_8_label tabindex=0> <span class=md-ellipsis> Terminal </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_7_8_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_7_8> <span class="md-nav__icon md-icon"></span> Terminal </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Tools/Terminal/Tabby_Zsh/ class=md-nav__link> <span class=md-ellipsis> Tabby + Zsh 配置指南 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_8 type=checkbox> <div class="md-nav__link md-nav__container"> <a href=../../../Tags/ class=md-nav__link> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M5.5 7A1.5 1.5 0 0 1 4 5.5 1.5 1.5 0 0 1 5.5 4 1.5 1.5 0 0 1 7 5.5 1.5 1.5 0 0 1 5.5 7m15.91 4.58-9-9C12.05 2.22 11.55 2 11 2H4c-1.11 0-2 .89-2 2v7c0 .55.22 1.05.59 1.41l8.99 9c.37.36.87.59 1.42.59s1.05-.23 1.41-.59l7-7c.37-.36.59-.86.59-1.41 0-.56-.23-1.06-.59-1.42"></path></svg> <span class=md-ellipsis> Tags </span> </a> </div> <nav aria-expanded=false aria-labelledby=__nav_8_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> Tags </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label="Table of contents" class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#1-introduction> <span class=md-ellipsis> 1 - Introduction </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#2-image-classification> <span class=md-ellipsis> 2 - Image Classification </span> </a> <nav aria-label="2 - Image Classification" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#k-nearest-neighbor> <span class=md-ellipsis> K Nearest Neighbor </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#linear-classifier> <span class=md-ellipsis> Linear Classifier </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#3-loss-functions-and-optimization> <span class=md-ellipsis> 3 - Loss Functions and Optimization </span> </a> <nav aria-label="3 - Loss Functions and Optimization" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#loss-functions> <span class=md-ellipsis> Loss Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#optimization> <span class=md-ellipsis> Optimization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#4-neural-networks-and-backpropagation> <span class=md-ellipsis> 4 - Neural Networks and Backpropagation </span> </a> <nav aria-label="4 - Neural Networks and Backpropagation" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#neural-networks> <span class=md-ellipsis> Neural Networks </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#backpropagation> <span class=md-ellipsis> Backpropagation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#5-convolutional-neural-networks> <span class=md-ellipsis> 5 - Convolutional Neural Networks </span> </a> <nav aria-label="5 - Convolutional Neural Networks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#convolution-layer> <span class=md-ellipsis> Convolution Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#pooling-layer> <span class=md-ellipsis> Pooling layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#convolutional-neural-networks-cnn> <span class=md-ellipsis> Convolutional Neural Networks (CNN) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#6-cnn-architectures> <span class=md-ellipsis> 6 - CNN Architectures </span> </a> <nav aria-label="6 - CNN Architectures" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#alexnet> <span class=md-ellipsis> AlexNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#vgg> <span class=md-ellipsis> VGG </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#googlenet> <span class=md-ellipsis> GoogLeNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#resnet> <span class=md-ellipsis> ResNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#senet> <span class=md-ellipsis> SENet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#improvements-of-resnet> <span class=md-ellipsis> Improvements of ResNet </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#other-interesting-networks> <span class=md-ellipsis> Other Interesting Networks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#7-training-neural-networks> <span class=md-ellipsis> 7 - Training Neural Networks </span> </a> <nav aria-label="7 - Training Neural Networks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#activation-functions> <span class=md-ellipsis> Activation Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-processing> <span class=md-ellipsis> Data Processing </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#weight-initialization> <span class=md-ellipsis> Weight Initialization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#batch-normalization> <span class=md-ellipsis> Batch Normalization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transfer-learning> <span class=md-ellipsis> Transfer Learning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#regularization> <span class=md-ellipsis> Regularization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#hyperparameter-tuning> <span class=md-ellipsis> Hyperparameter Tuning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#gradient-checks> <span class=md-ellipsis> Gradient Checks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#8-visualizing-and-understanding> <span class=md-ellipsis> 8 - Visualizing and Understanding </span> </a> <nav aria-label="8 - Visualizing and Understanding" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#feature-visualization-and-inversion> <span class=md-ellipsis> Feature Visualization and Inversion </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#adversarial-examples> <span class=md-ellipsis> Adversarial Examples </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#deepdream-and-style-transfer> <span class=md-ellipsis> DeepDream and Style Transfer </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#9-object-detection-and-image-segmentation> <span class=md-ellipsis> 9 - Object Detection and Image Segmentation </span> </a> <nav aria-label="9 - Object Detection and Image Segmentation" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#semantic-segmentation> <span class=md-ellipsis> Semantic Segmentation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#object-detection> <span class=md-ellipsis> Object Detection </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#instance-segmentation> <span class=md-ellipsis> Instance Segmentation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#10-recurrent-neural-networks> <span class=md-ellipsis> 10 - Recurrent Neural Networks </span> </a> <nav aria-label="10 - Recurrent Neural Networks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#recurrent-neural-network-rnn> <span class=md-ellipsis> Recurrent Neural Network (RNN) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#long-short-term-memory-lstm> <span class=md-ellipsis> Long Short Term Memory (LSTM) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#other-rnn-variants> <span class=md-ellipsis> Other RNN Variants </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#11-attention-and-transformers> <span class=md-ellipsis> 11 - Attention and Transformers </span> </a> <nav aria-label="11 - Attention and Transformers" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#rnn-with-attention> <span class=md-ellipsis> RNN with Attention </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#general-attention-layer> <span class=md-ellipsis> General Attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#self-attention-layer> <span class=md-ellipsis> Self-attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#positional-encoding> <span class=md-ellipsis> Positional Encoding </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#masked-self-attention-layer> <span class=md-ellipsis> Masked Self-attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#multi-head-self-attention-layer> <span class=md-ellipsis> Multi-head Self-attention Layer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transformer> <span class=md-ellipsis> Transformer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#comparing-rnns-to-transformer> <span class=md-ellipsis> Comparing RNNs to Transformer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#comparing-convnets-to-transformer> <span class=md-ellipsis> Comparing ConvNets to Transformer </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#12-video-understanding> <span class=md-ellipsis> 12 - Video Understanding </span> </a> <nav aria-label="12 - Video Understanding" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#video-classification> <span class=md-ellipsis> Video Classification </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#plain-cnn-structure> <span class=md-ellipsis> Plain CNN Structure </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#modeling-long-term-temporal-structure> <span class=md-ellipsis> Modeling Long-term Temporal Structure </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#visualizing-video-models> <span class=md-ellipsis> Visualizing Video Models </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#multimodal-video-understanding> <span class=md-ellipsis> Multimodal Video Understanding </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#13-generative-models> <span class=md-ellipsis> 13 - Generative Models </span> </a> <nav aria-label="13 - Generative Models" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#pixelrnn-and-pixelcnn> <span class=md-ellipsis> PixelRNN and PixelCNN </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#variational-autoencoder> <span class=md-ellipsis> Variational Autoencoder </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#generative-adversarial-networks-gans> <span class=md-ellipsis> Generative Adversarial Networks (GANs) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#14-self-supervised-learning> <span class=md-ellipsis> 14 - Self-supervised Learning </span> </a> <nav aria-label="14 - Self-supervised Learning" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#pretext-tasks> <span class=md-ellipsis> Pretext Tasks </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#contrastive-representation-learning> <span class=md-ellipsis> Contrastive Representation Learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#15-low-level-vision> <span class=md-ellipsis> 15 - Low-Level Vision </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#16-3d-vision> <span class=md-ellipsis> 16 - 3D Vision </span> </a> <nav aria-label="16 - 3D Vision" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#representation> <span class=md-ellipsis> Representation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#ai-3d> <span class=md-ellipsis> AI + 3D </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a class="md-content__button md-icon" href=https://github.com/zlflly/notes/edit/master/docs/AI/CS231n/CS231n_notes.md title=edit.link.title> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"></path></svg> </a> <h1 id=computer-vision>Computer Vision<a class=headerlink href=#computer-vision title="Permanent link">¶</a></h1> <div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;"> <p><span class=twemoji><svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约<span class=heti-skip><span class=heti-spacing> </span>8852<span class=heti-spacing> </span></span>个字 <span class=twemoji><svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M21 17H7V3h14m0-2H7a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2M3 5H1v16a2 2 0 0 0 2 2h16v-2H3m12.96-10.71-2.75 3.54-1.96-2.36L8.5 15h11z"></path></svg></span> <span>167<span class=heti-spacing> </span></span>张图片 <span class=twemoji><svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间<span class=heti-skip><span class=heti-spacing> </span>44<span class=heti-spacing> </span></span>分钟 <span class=twemoji><svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5"></path></svg></span> 共被读过 <span id=busuanzi_value_page_pv></span> 次</p> </div> <p><span>This note is based on<span class=heti-spacing> </span></span><a href=https://github.com/DaizeDong/Stanford-CS231n-2021-and-2022/ ><span>GitHub - DaizeDong/Stanford-CS231n-2021-and-2022: Notes and slides for Stanford CS231n 2021 &amp; 2022 in English. I merged the contents together to get a better version. Assignments are not included.<span class=heti-spacing> </span></span>斯坦福<span class=heti-skip><span class=heti-spacing> </span>cs231n<span class=heti-spacing> </span></span>的课程笔记<span class=heti-skip><span class=heti-spacing> </span>(<span class=heti-spacing> </span></span>英文版本，不含实验代码<span><span class=heti-spacing> </span>)</span>，将<span class=heti-skip><span class=heti-spacing> </span>2021<span class=heti-spacing> </span></span>与<span class=heti-skip><span class=heti-spacing> </span>2022<span class=heti-spacing> </span></span>两年的课程进行了合并，分享以供交流。</a><br> And I will add some blogs, articles and other understanding.</p> <table> <thead> <tr> <th>Topic</th> <th>Chapter</th> </tr> </thead> <tbody> <tr> <td>Deep Learning Basics</td> <td>2 - 4</td> </tr> <tr> <td>Perceiving and Understanding the Visual World</td> <td>5 - 12</td> </tr> <tr> <td>Reconstructing and Interacting with the Visual World</td> <td>13 - 16</td> </tr> <tr> <td>Human-Centered Applications and Implications</td> <td>17 - 18</td> </tr> </tbody> </table> <h2 id=1-introduction>1 - Introduction<a class=headerlink href=#1-introduction title="Permanent link">¶</a></h2> <p>A brief history of computer vision &amp; deep learning...</p> <h2 id=2-image-classification>2 - Image Classification<a class=headerlink href=#2-image-classification title="Permanent link">¶</a></h2> <p><strong>Image Classification:</strong> A core task in Computer Vision. The main drive to the progress of CV.</p> <p><strong>Challenges:</strong> Viewpoint variation, background clutter, illumination, occlusion, deformation, intra-class variation...</p> <h3 id=k-nearest-neighbor>K Nearest Neighbor<a class=headerlink href=#k-nearest-neighbor title="Permanent link">¶</a></h3> <p><strong>Hyperparameters:</strong> Distance metric (<span class=arithmatex>\(p\)</span> norm), <span class=arithmatex>\(k\)</span> number.</p> <p>Choose hyperparameters using validation set.</p> <p>Never use k-Nearest Neighbor with pixel distance.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/2-cross_validation.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/2-cross_validation.png></a></p> <h3 id=linear-classifier>Linear Classifier<a class=headerlink href=#linear-classifier title="Permanent link">¶</a></h3> <p>Pass...</p> <h2 id=3-loss-functions-and-optimization>3 - Loss Functions and Optimization<a class=headerlink href=#3-loss-functions-and-optimization title="Permanent link">¶</a></h2> <h3 id=loss-functions>Loss Functions<a class=headerlink href=#loss-functions title="Permanent link">¶</a></h3> <table> <thead> <tr> <th>Dataset</th> <th><span class=arithmatex>\(\big\{(x_i,y_i)\big\}_{i=1}^N\\\)</span></th> </tr> </thead> <tbody> <tr> <td>Loss Function</td> <td><span class=arithmatex>\(L=\frac{1}{N}\sum_{i=1}^NL_i\big(f(x_i,W),y_i\big)\\\)</span></td> </tr> <tr> <td>Loss Function with Regularization</td> <td><span class=arithmatex>\(L=\frac{1}{N}\sum_{i=1}^NL_i\big(f(x_i,W),y_i\big)+\lambda R(W)\\\)</span></td> </tr> </tbody> </table> <p><strong>Motivation:</strong> Want to interpret raw classifier scores as probabilities.</p> <table> <thead> <tr> <th>Softmax Classifier</th> <th><span class=arithmatex>\(p_i=Softmax(y_i)=\frac{\exp(y_i)}{\sum_{j=1}^N\exp(y_j)}\\\)</span></th> </tr> </thead> <tbody> <tr> <td>Cross Entropy Loss</td> <td><span class=arithmatex>\(L_i=-y_i\log p_i\\\)</span></td> </tr> <tr> <td>Cross Entropy Loss with Regularization</td> <td><span class=arithmatex>\(L=-\frac{1}{N}\sum_{i=1}^Ny_i\log p_i+\lambda R(W)\\\)</span></td> </tr> </tbody> </table> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-loss.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-loss.png></a></p> <h3 id=optimization>Optimization<a class=headerlink href=#optimization title="Permanent link">¶</a></h3> <h4 id=sgd-with-momentum>SGD with Momentum<a class=headerlink href=#sgd-with-momentum title="Permanent link">¶</a></h4> <p><strong>Problems that SGD can't handle:</strong></p> <ol> <li>Inequality of gradient in different directions.</li> <li>Local minima and saddle point (much more common in high dimension).</li> <li>Noise of gradient from mini-batch.</li> </ol> <p><strong>Momentum:</strong> Build up “velocity” <span class=arithmatex>\(v_t\)</span> as a running mean of gradients.</p> <table> <thead> <tr> <th>SGD</th> <th>SGD + Momentum</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(x_{t+1}=x_t-\alpha\nabla f(x_t)\)</span></td> <td><span class=arithmatex>\(\begin{align}&amp;v_{t+1}=\rho v_t+\nabla f(x_t)\\&amp;x_{t+1}=x_t-\alpha v_{t+1}\end{align}\)</span></td> </tr> <tr> <td>Naive gradient descent.</td> <td><span class=arithmatex>\(\rho\)</span> gives "friction", typically <span class=arithmatex>\(\rho=0.9,0.99,0.999,...\)</span></td> </tr> </tbody> </table> <p><strong>Nesterov Momentum:</strong> Use the derivative on point <span class=arithmatex>\(x_t+\rho v_t\)</span> as gradient instead point <span class=arithmatex>\(x_t\)</span>.</p> <table> <thead> <tr> <th>Momentum</th> <th>Nesterov Momentum</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(\begin{align}&amp;v_{t+1}=\rho v_t+\nabla f(x_t)\\&amp;x_{t+1}=x_t-\alpha v_{t+1}\end{align}\)</span></td> <td><span class=arithmatex>\(\begin{align}&amp;v_{t+1}=\rho v_t+\nabla f(x_t+\rho v_t)\\&amp;x_{t+1}=x_t-\alpha v_{t+1}\end{align}\)</span></td> </tr> <tr> <td>Use gradient at current point.</td> <td>Look ahead for the gradient in velocity direction.</td> </tr> </tbody> </table> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-momentum.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-momentum.png></a></p> <h4 id=adagrad-and-rmsprop>AdaGrad and RMSProp<a class=headerlink href=#adagrad-and-rmsprop title="Permanent link">¶</a></h4> <p><strong>AdaGrad:</strong> Accumulate squared gradient, and gradually decrease the step size.</p> <p><strong>RMSProp:</strong> Accumulate squared gradient while decaying former ones, and gradually decrease the step size. ("Leaky AdaGrad")</p> <table> <thead> <tr> <th>AdaGrad</th> <th>RMSProp</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(\begin{align}\text{Initialize:}&amp;\\&amp;r:=0\\\text{Update:}&amp;\\&amp;r:=r+\Big[\nabla f(x_t)\Big]^2\\&amp;x_{t+1}=x_t-\alpha\frac{\nabla f(x_t)}{\sqrt{r}}\end{align}\)</span></td> <td><span class=arithmatex>\(\begin{align}\text{Initialize:}&amp;\\&amp;r:=0\\\text{Update:}&amp;\\&amp;r:=\rho r+(1-\rho)\Big[\nabla f(x_t)\Big]^2\\&amp;x_{t+1}=x_t-\alpha\frac{\nabla f(x_t)}{\sqrt{r}}\end{align}\)</span></td> </tr> <tr> <td>Continually accumulate squared gradients.</td> <td><span class=arithmatex>\(\rho\)</span> gives "decay rate", typically <span class=arithmatex>\(\rho=0.9,0.99,0.999,...\)</span></td> </tr> </tbody> </table> <h4 id=adam>Adam<a class=headerlink href=#adam title="Permanent link">¶</a></h4> <p>Sort of like "RMSProp + Momentum".</p> <table> <thead> <tr> <th>Adam (simple version)</th> <th>Adam (full version)</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(\begin{align}\text{Initialize:}&amp;\\&amp;r_1:=0\\&amp;r_2:=0\\\text{Update:}&amp;\\&amp;r_1:=\beta_1r_1+(1-\beta_1)\nabla f(x_t)\\&amp;r_2:=\beta_2r_2+(1-\beta_2)\Big[\nabla f(x_t)\Big]^2\\&amp;x_{t+1}=x_t-\alpha\frac{r_1}{\sqrt{r_2}}\end{align}\)</span></td> <td><span class=arithmatex>\(\begin{align}\text{Initialize:}\\&amp;r_1:=0\\&amp;r_2:=0\\\text{For }i\text{:}\\&amp;r_1:=\beta_1r_1+(1-\beta_1)\nabla f(x_t)\\&amp;r_2:=\beta_2r_2+(1-\beta_2)\Big[\nabla f(x_t)\Big]^2\\&amp;r_1'=\frac{r_1}{1-\beta_1^i}\\&amp;r_2'=\frac{r_2}{1-\beta_2^i}\\&amp;x_{t+1}=x_t-\alpha\frac{r_1'}{\sqrt{r_2'}}\end{align}\)</span></td> </tr> <tr> <td>Build up “velocity” for both gradient and squared gradient.</td> <td>Correct the "bias" that <span class=arithmatex>\(r_1=r_2=0\)</span> for the first few iterations.</td> </tr> </tbody> </table> <h4 id=overview>Overview<a class=headerlink href=#overview title="Permanent link">¶</a></h4> <table> <thead> <tr> <th style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview.gif><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview.gif></a></th> <th style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview2.gif><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview2.gif></a></th> </tr> </thead> <tbody> <tr> <td></td> <td></td> </tr> </tbody> </table> <h4 id=learning-rate-decay>Learning Rate Decay<a class=headerlink href=#learning-rate-decay title="Permanent link">¶</a></h4> <p>Reduce learning rate at a few fixed points to get a better convergence over time.</p> <p><span class=arithmatex>\(\alpha_0\)</span> : Initial learning rate.</p> <p><span class=arithmatex>\(\alpha_t\)</span> : Learning rate in epoch <span class=arithmatex>\(t\)</span>.</p> <p><span class=arithmatex>\(T\)</span> : Total number of epochs.</p> <table> <thead> <tr> <th>Method</th> <th>Equation</th> <th>Picture</th> </tr> </thead> <tbody> <tr> <td>Step</td> <td>Reduce <span class=arithmatex>\(\alpha_t\)</span> constantly in a fixed step.</td> <td><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_step.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_step.png></a></td> </tr> <tr> <td>Cosine</td> <td><span class=arithmatex>\(\begin{align}\alpha_t=\frac{1}{2}\alpha_0\Bigg[1+\cos(\frac{t\pi}{T})\Bigg]\end{align}\)</span></td> <td><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_cosine.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_cosine.png></a></td> </tr> <tr> <td>Linear</td> <td><span class=arithmatex>\(\begin{align}\alpha_t=\alpha_0\Big(1-\frac{t}{T}\Big)\end{align}\)</span></td> <td><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_linear.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_linear.png></a></td> </tr> <tr> <td>Inverse Sqrt</td> <td><span class=arithmatex>\(\begin{align}\alpha_t=\frac{\alpha_0}{\sqrt{t}}\end{align}\)</span></td> <td><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_sqrt.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_sqrt.png></a></td> </tr> </tbody> </table> <p>High initial learning rates can make loss explode, linearly increasing learning rate in the first few iterations can prevent this.</p> <p><strong>Learning rate warm up:</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_increase.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_increase.png></a></p> <p><strong>Empirical rule of thumb:</strong> If you increase the batch size by <span class=arithmatex>\(N\)</span>, also scale the initial learning rate by <span class=arithmatex>\(N\)</span> .</p> <h4 id=second-order-optimization>Second-Order Optimization<a class=headerlink href=#second-order-optimization title="Permanent link">¶</a></h4> <table> <thead> <tr> <th></th> <th>Picture</th> <th>Time Complexity</th> <th>Space Complexity</th> </tr> </thead> <tbody> <tr> <td>First Order</td> <td><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-first_order.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-first_order.png></a></td> <td><span class=arithmatex>\(O(n)\)</span></td> <td><span class=arithmatex>\(O(n)\)</span></td> </tr> <tr> <td>Second Order</td> <td><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-second_order.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/3-second_order.png></a></td> <td><span class=arithmatex>\(O(n^2)\)</span> with <strong>BGFS</strong> optimization</td> <td><span class=arithmatex>\(O(n)\)</span> with <strong>L-BGFS</strong> optimization</td> </tr> </tbody> </table> <p><strong>L-BGFS :</strong> Limited memory BGFS.</p> <ol> <li>Works very well in full batch, deterministic <span class=arithmatex>\(f(x)\)</span>.</li> <li>Does not transfer very well to mini-batch setting.</li> </ol> <h4 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">¶</a></h4> <table> <thead> <tr> <th>Method</th> <th>Performance</th> </tr> </thead> <tbody> <tr> <td>Adam</td> <td>Often chosen as default method.<br>Work ok even with constant learning rate.</td> </tr> <tr> <td>SGD + Momentum</td> <td>Can outperform Adam.<br>Require more tuning of learning rate and schedule.</td> </tr> <tr> <td>L-BGFS</td> <td>If can afford to do full batch updates then try out.</td> </tr> </tbody> </table> <ul> <li>An article about gradient descent: <a href=https://arxiv.org/pdf/1609.04747>Anoverview of gradient descent optimization algorithms</a></li> <li>A blog: <a href=https://johnchenresearch.github.io/demon/ >An updated overview of recent gradient descent algorithms – John Chen – ML at Rice University</a></li> </ul> <h2 id=4-neural-networks-and-backpropagation>4 - Neural Networks and Backpropagation<a class=headerlink href=#4-neural-networks-and-backpropagation title="Permanent link">¶</a></h2> <h3 id=neural-networks>Neural Networks<a class=headerlink href=#neural-networks title="Permanent link">¶</a></h3> <p><strong>Motivation:</strong> Inducted bias can appear to be high when using human-designed features.</p> <p><strong>Activation:</strong> Sigmoid, tanh, ReLU, LeakyReLU...</p> <p><strong>Architecture:</strong> Input layer, hidden layer, output layer.</p> <p><strong>Do not use the size of a neural network as the regularizer. Use regularization instead!</strong></p> <p><strong>Gradient Calculation:</strong> Computational Graph + Backpropagation.</p> <h3 id=backpropagation>Backpropagation<a class=headerlink href=#backpropagation title="Permanent link">¶</a></h3> <p>Using Jacobian matrix to calculate the gradient of each node in a computation graph.</p> <p>Suppose that we have a computation flow like this:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph.png></a></p> <table> <thead> <tr> <th>Input X</th> <th>Input W</th> <th>Output Y</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(X=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}\)</span></td> <td><span class=arithmatex>\(W=\begin{bmatrix}w_{11}&amp;w_{12}&amp;\cdots&amp;w_{1n}\\w_{21}&amp;w_{22}&amp;\cdots&amp;w_{2n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\w_{m1}&amp;w_{m2}&amp;\cdots&amp;w_{mn}\end{bmatrix}\)</span></td> <td><span class=arithmatex>\(Y=\begin{bmatrix}y_1\\y_2\\\vdots\\y_m\end{bmatrix}\)</span></td> </tr> <tr> <td><span class=arithmatex>\(n\times 1\)</span></td> <td><span class=arithmatex>\(m\times n\)</span></td> <td><span class=arithmatex>\(m\times 1\)</span></td> </tr> </tbody> </table> <p>After applying feed forward, we can calculate gradients like this:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph2.png></a></p> <table> <thead> <tr> <th>Derivative Matrix of X</th> <th>Jacobian Matrix of X</th> <th>Derivative Matrix of Y</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(D_X=\begin{bmatrix}\frac{\partial L}{\partial x_1}\\\frac{\partial L}{\partial x_2}\\\vdots\\\frac{\partial L}{\partial x_n}\end{bmatrix}\)</span></td> <td><span class=arithmatex>\(J_X=\begin{bmatrix}\frac{\partial y_1}{\partial x_1}&amp;\frac{\partial y_1}{\partial x_2}&amp;\cdots&amp;\frac{\partial y_1}{\partial x_n}\\\frac{\partial y_2}{\partial x_1}&amp;\frac{\partial y_2}{\partial x_2}&amp;\cdots&amp;\frac{\partial y_2}{\partial x_n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial y_m}{\partial x_1}&amp;\frac{\partial y_m}{\partial x_2}&amp;\cdots&amp;\frac{\partial y_m}{\partial x_n}\end{bmatrix}\)</span></td> <td><span class=arithmatex>\(D_Y=\begin{bmatrix}\frac{\partial L}{\partial y_1}\\\frac{\partial L}{\partial y_2}\\\vdots\\\frac{\partial L}{\partial y_m}\end{bmatrix}\)</span></td> </tr> <tr> <td><span class=arithmatex>\(n\times 1\)</span></td> <td><span class=arithmatex>\(m\times n\)</span></td> <td><span class=arithmatex>\(m\times 1\)</span></td> </tr> </tbody> </table> <table> <thead> <tr> <th>Derivative Matrix of W</th> <th>Jacobian Matrix of W</th> <th>Derivative Matrix of Y</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(W=\begin{bmatrix}\frac{\partial L}{\partial w_{11}}&amp;\frac{\partial L}{\partial w_{12}}&amp;\cdots&amp;\frac{\partial L}{\partial w_{1n}}\\\frac{\partial L}{\partial w_{21}}&amp;\frac{\partial L}{\partial w_{22}}&amp;\cdots&amp;\frac{\partial L}{\partial w_{2n}}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial L}{\partial w_{m1}}&amp;\frac{\partial L}{\partial w_{m2}}&amp;\cdots&amp;\frac{\partial L}{\partial w_{mn}}\end{bmatrix}\)</span></td> <td><span class=arithmatex>\(J_W^{(k)}=\begin{bmatrix}\frac{\partial y_k}{\partial w_{11}}&amp;\frac{\partial y_k}{\partial w_{12}}&amp;\cdots&amp;\frac{\partial y_k}{\partial w_{1n}}\\\frac{\partial y_k}{\partial w_{21}}&amp;\frac{\partial y_k}{\partial w_{22}}&amp;\cdots&amp;\frac{\partial y_k}{\partial w_{2n}}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial y_k}{\partial w_{m1}}&amp;\frac{\partial y_k}{\partial w_{m2}}&amp;\cdots&amp;\frac{\partial y_k}{\partial w_{mn}}\end{bmatrix}\)</span><br><span class=arithmatex>\(J_W=\begin{bmatrix}J_W^{(1)}&amp;J_W^{(2)}&amp;\cdots&amp;J_W^{(m)}\end{bmatrix}\)</span></td> <td><span class=arithmatex>\(D_Y=\begin{bmatrix}\frac{\partial L}{\partial y_1}\\\frac{\partial L}{\partial y_2}\\\vdots\\\frac{\partial L}{\partial y_m}\end{bmatrix}\)</span></td> </tr> <tr> <td><span class=arithmatex>\(m\times n\)</span></td> <td><span class=arithmatex>\(m\times m\times n\)</span></td> <td>$ m\times 1$</td> </tr> </tbody> </table> <p>For each element in <span class=arithmatex>\(D_X\)</span> , we have:</p> <p><span class=arithmatex>\(D_{Xi}=\frac{\partial L}{\partial x_i}=\sum_{j=1}^m\frac{\partial L}{\partial y_j}\frac{\partial y_j}{\partial x_i}\\\)</span></p> <h2 id=5-convolutional-neural-networks>5 - Convolutional Neural Networks<a class=headerlink href=#5-convolutional-neural-networks title="Permanent link">¶</a></h2> <h3 id=convolution-layer>Convolution Layer<a class=headerlink href=#convolution-layer title="Permanent link">¶</a></h3> <h4 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">¶</a></h4> <p><strong>Convolve a filter with an image:</strong> Slide the filter spatially within the image, computing dot products in each region.</p> <p>Giving a <span class=arithmatex>\(32\times32\times3\)</span> image and a <span class=arithmatex>\(5\times5\times3\)</span> filter, a convolution looks like:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution.png></a></p> <p>Convolve six <span class=arithmatex>\(5\times5\times3\)</span> filters to a <span class=arithmatex>\(32\times32\times3\)</span> image with step size <span class=arithmatex>\(1\)</span>, we can get a <span class=arithmatex>\(28\times28\times6\)</span> feature:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_six_filters.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_six_filters.png></a></p> <p>With an activation function after each convolution layer, we can build the ConvNet with a sequence of convolution layers:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_net.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_net.png></a></p> <p>By <strong>changing the step size</strong> between each move for filters, or <strong>adding zero-padding</strong> around the image, we can modify the size of the output:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_padding.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_padding.png></a></p> <h4 id=1times1-convolution-layer><span class=arithmatex>\(1\times1\)</span> Convolution Layer<a class=headerlink href=#1times1-convolution-layer title="Permanent link">¶</a></h4> <p>This kind of layer makes perfect sense. It is usually used to change the dimension (channel) of features.</p> <p>A <span class=arithmatex>\(1\times1\)</span> convolution layer can also be treated as a full-connected linear layer.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_1times1.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_1times1.png></a></p> <h4 id=summary_1>Summary<a class=headerlink href=#summary_1 title="Permanent link">¶</a></h4> <table> <thead> <tr> <th><strong>Input</strong></th> <th></th> </tr> </thead> <tbody> <tr> <td>image size</td> <td><span class=arithmatex>\(W_1\times H_1\times C\)</span></td> </tr> <tr> <td>filter size</td> <td><span class=arithmatex>\(F\times F\times C\)</span></td> </tr> <tr> <td>filter number</td> <td><span class=arithmatex>\(K\)</span></td> </tr> <tr> <td>stride</td> <td><span class=arithmatex>\(S\)</span></td> </tr> <tr> <td>zero padding</td> <td><span class=arithmatex>\(P\)</span></td> </tr> <tr> <td><strong>Output</strong></td> <td></td> </tr> <tr> <td>output size</td> <td><span class=arithmatex>\(W_2\times H_2\times K\)</span></td> </tr> <tr> <td>output width</td> <td><span class=arithmatex>\(W_2=\frac{W_1-F+2P}{S}+1\\\)</span></td> </tr> <tr> <td>output height</td> <td><span class=arithmatex>\(H_2=\frac{H_1-F+2P}{S}+1\\\)</span></td> </tr> <tr> <td><strong>Parameters</strong></td> <td></td> </tr> <tr> <td>parameter number (weight)</td> <td><span class=arithmatex>\(F^2CK\)</span></td> </tr> <tr> <td>parameter number (bias)</td> <td><span class=arithmatex>\(K\)</span></td> </tr> </tbody> </table> <h3 id=pooling-layer>Pooling layer<a class=headerlink href=#pooling-layer title="Permanent link">¶</a></h3> <p>Make the representations smaller and more manageable.</p> <p><strong>An example of max pooling:</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-pooling.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-pooling.png></a></p> <table> <thead> <tr> <th><strong>Input</strong></th> <th></th> </tr> </thead> <tbody> <tr> <td>image size</td> <td><span class=arithmatex>\(W_1\times H_1\times C\)</span></td> </tr> <tr> <td>spatial extent</td> <td><span class=arithmatex>\(F\times F\)</span></td> </tr> <tr> <td>stride</td> <td><span class=arithmatex>\(S\)</span></td> </tr> <tr> <td><strong>Output</strong></td> <td></td> </tr> <tr> <td>output size</td> <td><span class=arithmatex>\(W_2\times H_2\times C\)</span></td> </tr> <tr> <td>output width</td> <td><span class=arithmatex>\(W_2=\frac{W_1-F}{S}+1\\\)</span></td> </tr> <tr> <td>output height</td> <td><span class=arithmatex>\(H_2=\frac{H_1-F}{S}+1\\\)</span></td> </tr> </tbody> </table> <h3 id=convolutional-neural-networks-cnn>Convolutional Neural Networks (CNN)<a class=headerlink href=#convolutional-neural-networks-cnn title="Permanent link">¶</a></h3> <p>CNN stack CONV, POOL, FC layers.</p> <p><strong>CNN Trends:</strong></p> <ol> <li>Smaller filters and deeper architectures.</li> <li>Getting rid of POOL/FC layers (just CONV).</li> </ol> <p><strong>Historically architectures of CNN looked like:</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-model_history.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/5-model_history.png></a></p> <p>where usually <span class=arithmatex>\(m\)</span> is large, <span class=arithmatex>\(0\le n\le5\)</span>, <span class=arithmatex>\(0\le k\le2\)</span>.</p> <p>Recent advances such as <strong>ResNet</strong> / <strong>GoogLeNet</strong> have challenged this paradigm.</p> <h2 id=6-cnn-architectures>6 - CNN Architectures<a class=headerlink href=#6-cnn-architectures title="Permanent link">¶</a></h2> <p>Best model in ImageNet competition:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-image_net.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-image_net.png></a></p> <h3 id=alexnet>AlexNet<a class=headerlink href=#alexnet title="Permanent link">¶</a></h3> <p>8 layers.</p> <p>First use of ConvNet in image classification problem.</p> <p>Filter size decreases in deeper layer.</p> <p>Channel number increases in deeper layer.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet_p.png></a></p> <h3 id=vgg>VGG<a class=headerlink href=#vgg title="Permanent link">¶</a></h3> <p>19 layers. (also provide 16 layers edition)</p> <p>Static filter size (<span class=arithmatex>\(3\times3\)</span>) in all layers:</p> <ol> <li>The effective receptive field expands with the layer gets deeper.</li> <li>Deeper architecture gets more non-linearities and few parameters.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_field.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_field.png></a></p> <p>Most memory is in early convolution layers.</p> <p>Most parameter is in late FC layers.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_p.png></a></p> <h3 id=googlenet>GoogLeNet<a class=headerlink href=#googlenet title="Permanent link">¶</a></h3> <p>22 layers.</p> <p>No FC layers, only 5M parameters. ( <span class=arithmatex>\(8.3\%\)</span> of AlexNet, <span class=arithmatex>\(3.7\%\)</span> of VGG )</p> <p>Devise efficient "inception module".</p> <h4 id=inception-module>Inception Module<a class=headerlink href=#inception-module title="Permanent link">¶</a></h4> <p>Design a good local network topology (network within a network) and then stack these modules on top of each other.</p> <p><strong>Naive Inception Module:</strong></p> <ol> <li>Apply parallel filter operations on the input from previous layer.</li> <li>Concatenate all filter outputs together channel-wise.</li> <li><strong>Problem:</strong> The depth (channel number) increases too fast, costing expensive computation.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception.png></a></p> <p><strong>Inception Module with Dimension Reduction:</strong></p> <ol> <li>Add "bottle neck" layers to reduce the dimension.</li> <li>Also get fewer computation cost.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception_revised.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception_revised.png></a></p> <h4 id=architecture>Architecture<a class=headerlink href=#architecture title="Permanent link">¶</a></h4> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p2.png></a></p> <h3 id=resnet>ResNet<a class=headerlink href=#resnet title="Permanent link">¶</a></h3> <p>152 layers for ImageNet.</p> <p>Devise "residual connections".</p> <p>Use BN in place of dropout.</p> <h4 id=residual-connections>Residual Connections<a class=headerlink href=#residual-connections title="Permanent link">¶</a></h4> <p><strong>Hypothesis:</strong> Deeper models have more representation power than shallow ones. But they are harder to optimize.</p> <p><strong>Solution:</strong> Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_residual.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_residual.png></a></p> <p>It is necessary to use ReLU as activation function, in order to apply identity mapping when <span class=arithmatex>\(F(x)=0\)</span> .</p> <h4 id=architecture_1>Architecture<a class=headerlink href=#architecture_1 title="Permanent link">¶</a></h4> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_train.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_train.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_p.png></a></p> <h3 id=senet>SENet<a class=headerlink href=#senet title="Permanent link">¶</a></h3> <p>Using ResNeXt-152 as a base architecture.</p> <p>Add a “feature recalibration” module. <strong>(adjust weights of each channel)</strong></p> <p>Using the <strong>global avg-pooling layer</strong> + <strong>FC layers</strong> to determine feature map weights.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p2.png></a></p> <h3 id=improvements-of-resnet>Improvements of ResNet<a class=headerlink href=#improvements-of-resnet title="Permanent link">¶</a></h3> <p>Wide Residual Networks, ResNeXt, DenseNet, MobileNets...</p> <h3 id=other-interesting-networks>Other Interesting Networks<a class=headerlink href=#other-interesting-networks title="Permanent link">¶</a></h3> <p><strong>NASNet:</strong> Neural Architecture Search with Reinforcement Learning.</p> <p><strong>EfficientNet:</strong> Smart Compound Scaling.</p> <h2 id=7-training-neural-networks>7 - Training Neural Networks<a class=headerlink href=#7-training-neural-networks title="Permanent link">¶</a></h2> <h3 id=activation-functions>Activation Functions<a class=headerlink href=#activation-functions title="Permanent link">¶</a></h3> <table> <thead> <tr> <th>Activation</th> <th>Usage</th> </tr> </thead> <tbody> <tr> <td>Sigmoid, tanh</td> <td>Do not use.</td> </tr> <tr> <td>ReLU</td> <td>Use as default.</td> </tr> <tr> <td>Leaky ReLU, Maxout, ELU, SELU</td> <td>Replace ReLU to squeeze out some marginal gains.</td> </tr> <tr> <td>Swish</td> <td>No clear usage.</td> </tr> </tbody> </table> <h3 id=data-processing>Data Processing<a class=headerlink href=#data-processing title="Permanent link">¶</a></h3> <p>Apply centralization and normalization before training.</p> <p>In practice for pictures, usually we apply channel-wise centralization only.</p> <h3 id=weight-initialization>Weight Initialization<a class=headerlink href=#weight-initialization title="Permanent link">¶</a></h3> <p>Assume that we have 6 layers in a network.</p> <p><span class=arithmatex>\(D_i\)</span> : input size of layer <span class=arithmatex>\(i\)</span></p> <p><span class=arithmatex>\(W_i\)</span> : weights in layer <span class=arithmatex>\(i\)</span></p> <p><span class=arithmatex>\(X_i\)</span> : output after activation of layer <span class=arithmatex>\(i\)</span>, we have <span class=arithmatex>\(X_i=g(Z_i)=g(W_iX_{i-1}+B_i)\)</span></p> <p><strong>We initialize each parameter in <span class=arithmatex>\(W_i\)</span> randomly in <span class=arithmatex>\([-k_i,k_i]\)</span> .</strong></p> <table> <thead> <tr> <th style="text-align: center;">Tanh Activation</th> <th style="text-align: center;">Output Distribution</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;"><span class=arithmatex>\(k_i=0.01\)</span></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.01.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.01.png></a></td> </tr> <tr> <td style="text-align: center;"><span class=arithmatex>\(k_i=0.05\)</span></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.05.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.05.png></a></td> </tr> <tr> <td style="text-align: center;"><strong>Xavier Initialization</strong> <span class=arithmatex>\(k_i=\frac{1}{\sqrt{D_i}\\}\)</span></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_xavier.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_xavier.png></a></td> </tr> </tbody> </table> <p>When <span class=arithmatex>\(k_i=0.01\)</span>, the variance keeps decreasing as the layer gets deeper. As a result, the output of each neuron in deep layer will all be 0. The partial derivative <span class=arithmatex>\(\frac{\partial Z_i}{\partial W_i}=X_{i-1}=0\\\)</span>. (no gradient)</p> <p>When <span class=arithmatex>\(k_i=0.05\)</span>, most neurons is saturated. The partial derivative <span class=arithmatex>\(\frac{\partial X_i}{\partial Z_i}=g'(Z_i)=0\\\)</span>. (no gradient)</p> <p><strong>To solve this problem, We need to keep the variance same in each layer.</strong></p> <p>Assuming that <span class=arithmatex>\(Var\big(X_{i-1}^{(1)}\big)=Var\big(X_{i-1}^{(2)}\big)=\dots=Var\big(X_{i-1}^{(D_i)}\big)\)</span></p> <p>We have <span class=arithmatex>\(Z_i=X_{i-1}^{(1)}W_i^{(:,1)}+X_{i-1}^{(2)}W_i^{(:,2)}+\dots+X_{i-1}^{(D_i)}W_i^{(:,D_i)}=\sum_{n=1}^{D_i}X_{i-1}^{(n)}W_i^{(:,n)}\\\)</span></p> <p>We want <span class=arithmatex>\(Var\big(Z_i\big)=Var\big(X_{i-1}^{(n)}\big)\)</span></p> <p><strong>Let's do some conduction:</strong></p> <p><span class=arithmatex>\(\begin{aligned}Var\big(Z_i\big)&amp;=Var\Bigg(\sum_{n=1}^{D_i}X_{i-1}^{(n)}W_i^{(:,n)}\Bigg)\\&amp;=D_i\ Var\Big(X_{i-1}^{(n)}W_i^{(:,n)}\Big)\\&amp;=D_i\ Var\Big(X_{i-1}^{(n)}\Big)\ Var\Big(W_i^{(:,n)}\Big)\end{aligned}\)</span></p> <p>So <span class=arithmatex>\(Var\big(Z_i\big)=Var\big(X_{i-1}^{(n)}\big)\)</span> only when <span class=arithmatex>\(Var\Big(W_i^{(:,n)}\Big)=\frac{1}{D_i}\\\)</span>, that is to say <span class=arithmatex>\(k_i=\frac{1}{\sqrt{D_i}}\\\)</span></p> <table> <thead> <tr> <th style="text-align: center;">ReLU Activation</th> <th style="text-align: center;">Output Distribution</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;"><strong>Xavier Initialization</strong> <span class=arithmatex>\(k_i=\frac{1}{\sqrt{D_i}\\}\)</span></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_xavier.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_xavier.png></a></td> </tr> <tr> <td style="text-align: center;"><strong>Kaiming Initialization</strong> <span class=arithmatex>\(k_i=\sqrt{2D_i}\)</span></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_kaiming.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_kaiming.png></a></td> </tr> </tbody> </table> <p>For ReLU activation, when using xavier initialization, there still exist "variance decreasing" problem.</p> <p>We can use kaiming initialization instead to fix this.</p> <h3 id=batch-normalization>Batch Normalization<a class=headerlink href=#batch-normalization title="Permanent link">¶</a></h3> <p>Force the inputs to be "nicely scaled" at each layer.</p> <p><span class=arithmatex>\(N\)</span> : batch size</p> <p><span class=arithmatex>\(D\)</span> : feature size</p> <p><span class=arithmatex>\(x\)</span> : input with shape <span class=arithmatex>\(N\times D\)</span> </p> <p><span class=arithmatex>\(\gamma\)</span> : learnable scale and shift parameter with shape <span class=arithmatex>\(D\)</span></p> <p><span class=arithmatex>\(\beta\)</span> : learnable scale and shift parameter with shape <span class=arithmatex>\(D\)</span></p> <p><strong>The procedure of batch normalization:</strong></p> <ol> <li>Calculate channel-wise mean <span class=arithmatex>\(\mu_j=\frac{1}{N}\sum_{i=1}^Nx_{i,j}\\\)</span> . The result <span class=arithmatex>\(\mu\)</span> with shape <span class=arithmatex>\(D\)</span> .</li> <li>Calculate channel-wise variance <span class=arithmatex>\(\sigma_j^2=\frac{1}{N}\sum_{i=1}^N(x_{i,j}-\mu_j)^2\\\)</span> . The result <span class=arithmatex>\(\sigma^2\)</span> with shape <span class=arithmatex>\(D\)</span> .</li> <li>Calculate normalized <span class=arithmatex>\(\hat{x}_{i,j}=\frac{x_{i,j}-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}\\\)</span> . The result <span class=arithmatex>\(\hat{x}\)</span> with shape <span class=arithmatex>\(N\times D\)</span> .</li> <li>Scale normalized input to get output <span class=arithmatex>\(y_{i,j}=\gamma_j\hat{x}_{i,j}+\beta_j\)</span> . The result <span class=arithmatex>\(y\)</span> with shape <span class=arithmatex>\(N\times D\)</span> .</li> </ol> <p><strong>Why scale:</strong> The constraint "zero-mean, unit variance" may be too hard.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-batch_norm.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-batch_norm.png></a></p> <p><strong>Pros:</strong></p> <ol> <li>Makes deep networks much easier to train!</li> <li>Improves gradient flow.</li> <li>Allows higher learning rates, faster convergence.</li> <li>Networks become more robust to initialization.</li> <li>Acts as regularization during training.</li> <li>Zero overhead at test-time: can be fused with conv!</li> </ol> <p><strong>Cons:</strong></p> <p>Behaves differently during training and testing: this is a very common source of bugs!</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-all_norm.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-all_norm.png></a></p> <h3 id=transfer-learning>Transfer Learning<a class=headerlink href=#transfer-learning title="Permanent link">¶</a></h3> <p>Train on a pre-trained model with other datasets.</p> <p><strong>An empirical suggestion:</strong></p> <table> <thead> <tr> <th></th> <th><strong>very similar dataset</strong></th> <th><strong>very different dataset</strong></th> </tr> </thead> <tbody> <tr> <td><strong>very little data</strong></td> <td>Use Linear Classifier on top layer.</td> <td>You’re in trouble… Try linear classifier from different stages.</td> </tr> <tr> <td><strong>quite a lot of data</strong></td> <td>Finetune a few layers.</td> <td>Finetune a larger number of layers.</td> </tr> </tbody> </table> <h3 id=regularization>Regularization<a class=headerlink href=#regularization title="Permanent link">¶</a></h3> <h4 id=common-pattern-of-regularization>Common Pattern of Regularization<a class=headerlink href=#common-pattern-of-regularization title="Permanent link">¶</a></h4> <p>Training: Add some kind of randomness. <span class=arithmatex>\(y=f(x,z)\)</span></p> <p>Testing: Average out randomness (sometimes approximate). <span class=arithmatex>\(y=f(x)=E_z\big[f(x,z)\big]=\int p(z)f(x,z)dz\\\)</span></p> <h4 id=regularization-term>Regularization Term<a class=headerlink href=#regularization-term title="Permanent link">¶</a></h4> <p>L2 regularization: <span class=arithmatex>\(R(W)=\sum_k\sum_lW_{k,l}^2\)</span> (weight decay)</p> <p>L1 regularization: <span class=arithmatex>\(R(W)=\sum_k\sum_l|W_{k,l}|\)</span></p> <p>Elastic net : <span class=arithmatex>\(R(W)=\sum_k\sum_l\big(\beta W_{k,l}^2+|W_{k,l}|\big)\)</span> (L1+L2)</p> <h4 id=dropout>Dropout<a class=headerlink href=#dropout title="Permanent link">¶</a></h4> <p>Training: Randomly set some neurons to 0 with a probability <span class=arithmatex>\(p\)</span> .</p> <p>Testing: Each neuron multiplies by dropout probability <span class=arithmatex>\(p\)</span> . (scale the output back)</p> <p><strong>More common:</strong> Scale the output with <span class=arithmatex>\(\frac{1}{p}\)</span> when training, keep the original output when testing.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout_p.png></a></p> <p><strong>Why dropout works:</strong></p> <ol> <li>Forces the network to have a redundant representation. Prevents co-adaptation of features.</li> <li><strong>Another interpretation:</strong> Dropout is training a large ensemble of models (that share parameters).</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout.png></a></p> <h4 id=batch-normalization_1>Batch Normalization<a class=headerlink href=#batch-normalization_1 title="Permanent link">¶</a></h4> <p>See above.</p> <h4 id=data-augmentation>Data Augmentation<a class=headerlink href=#data-augmentation title="Permanent link">¶</a></h4> <ol> <li>Horizontal Flips</li> <li>Random Crops and Scales</li> <li>Color Jitter</li> <li>Rotation</li> <li>Stretching</li> <li>Shearing</li> <li>Lens Distortions</li> <li>...</li> </ol> <p>There also exists automatic data augmentation method using neural networks.</p> <h4 id=other-methods-and-summary>Other Methods and Summary<a class=headerlink href=#other-methods-and-summary title="Permanent link">¶</a></h4> <p><strong>DropConnect</strong>: Drop connections between neurons.</p> <p><strong>Fractional Max Pooling:</strong> Use randomized pooling regions.</p> <p><strong>Stochastic Depth</strong>: Skip some layers in the network.</p> <p><strong>Cutout:</strong> Set random image regions to zero.</p> <p><strong>Mixup:</strong> Train on random blends of images.</p> <table> <thead> <tr> <th>Regularization Method</th> <th>Usage</th> </tr> </thead> <tbody> <tr> <td>Dropout</td> <td>For large fully-connected layers.</td> </tr> <tr> <td>Batch Normalization &amp; Data Augmentation</td> <td>Almost always a good idea.</td> </tr> <tr> <td>Cutout &amp; Mixup</td> <td>For small classification datasets.</td> </tr> </tbody> </table> <h3 id=hyperparameter-tuning>Hyperparameter Tuning<a class=headerlink href=#hyperparameter-tuning title="Permanent link">¶</a></h3> <table> <thead> <tr> <th>Most Common Hyperparameters</th> <th>Less Sensitive Hyperparameters</th> </tr> </thead> <tbody> <tr> <td>learning rate<br>learning rate decay schedule<br>weight decay</td> <td>setting of momentum<br>...</td> </tr> </tbody> </table> <p><strong>Tips on hyperparameter tuning:</strong></p> <ol> <li>Prefer one validation fold to cross-validation.</li> <li>Search for hyperparameters on log scale. (e.g. multiply the hyperparameter by a fixed number <span class=arithmatex>\(k\)</span> at each search)</li> <li>Prefer <strong>random search</strong> to grid search.</li> <li>Careful with best values on border.</li> <li>Stage your search from coarse to fine.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-random_search.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/7-random_search.png></a></p> <h4 id=implementation>Implementation<a class=headerlink href=#implementation title="Permanent link">¶</a></h4> <p>Have a <strong>worker</strong> that continuously samples random hyperparameters and performs the optimization. During the training, the worker will keep track of the validation performance after every epoch, and writes a model checkpoint to a file.</p> <p>Have a <strong>master</strong> that launches or kills workers across a computing cluster, and may additionally inspect the checkpoints written by workers and plot their training statistics.</p> <h4 id=common-procedures>Common Procedures<a class=headerlink href=#common-procedures title="Permanent link">¶</a></h4> <ol> <li><strong>Check initial loss.</strong></li> </ol> <p>Turn off weight decay, sanity check loss at initialization <span class=arithmatex>\(\log(C)\)</span> for softmax with <span class=arithmatex>\(C\)</span> classes.</p> <ol start=2> <li><strong>Overfit a small sample. (important)</strong></li> </ol> <p>Try to train to 100% training accuracy on a small sample of training data.</p> <p>Fiddle with architecture, learning rate, weight initialization.</p> <ol start=3> <li><strong>Find learning rate that makes loss go down.</strong></li> </ol> <p>Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within 100 iterations.</p> <p>Good learning rates to try: <span class=arithmatex>\(0.1,0.01,0.001,0.0001,\dots\)</span></p> <ol start=4> <li><strong>Coarse grid, train for 1-5 epochs.</strong></li> </ol> <p>Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for 1-5 epochs.\</p> <p>Good weight decay to try: <span class=arithmatex>\(0.0001,0.00001,0\)</span></p> <ol start=5> <li><strong>Refine grid, train longer.</strong></li> </ol> <p>Pick best models from Step 4, train them for longer (10-20 epochs) without learning rate decay.</p> <ol start=6> <li><strong>Look at loss and accuracy curves.</strong></li> <li><strong>GOTO step 5.</strong></li> </ol> <h3 id=gradient-checks>Gradient Checks<a class=headerlink href=#gradient-checks title="Permanent link">¶</a></h3> <p><a href=https://cs231n.github.io/neural-networks-3/#gradcheck>CS231n Convolutional Neural Networks for Visual Recognition</a></p> <p>Compute analytical gradient manually using <span class=arithmatex>\(f_a'=\frac{\partial f(x)}{\partial x}=\frac{f(x-h)-f(x+h)}{2h}\\\)</span></p> <p>Get relative error between numerical gradient <span class=arithmatex>\(f_n'\)</span> and analytical gradient <span class=arithmatex>\(f_a'\)</span> using <span class=arithmatex>\(E=\frac{|f_n'-f_a'|}{\max{|f_n'|,|f_a'|}}\\\)</span></p> <table> <thead> <tr> <th>Relative Error</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(E&gt;10^{-2}\)</span></td> <td>Probably <span class=arithmatex>\(f_n'\)</span> is wrong.</td> </tr> <tr> <td><span class=arithmatex>\(10^{-2}&gt;E&gt;10^{-4}\)</span></td> <td>Not good, should check the gradient.</td> </tr> <tr> <td><span class=arithmatex>\(10^{-4}&gt;E&gt;10^{-6}\)</span></td> <td>Okay for objectives with kinks. (e.g. ReLU)<br>Not good for objectives with no kink. (e.g. softmax, tanh)</td> </tr> <tr> <td><span class=arithmatex>\(10^{-7}&gt;E\)</span></td> <td>Good.</td> </tr> </tbody> </table> <p><strong>Tips on gradient checks:</strong></p> <ol> <li>Use double precision.</li> <li>Use only few data points.</li> <li>Careful about kinks in the objective. (e.g. <span class=arithmatex>\(x=0\)</span> for ReLU activation)</li> <li>Careful with the step size <span class=arithmatex>\(h\)</span>.</li> <li>Use gradient check after the loss starts to go down.</li> <li>Remember to turn off anything that may affect the gradient. (e.g. <strong>regularization / dropout / augmentations</strong>)</li> <li>Check only few dimensions for <strong>every parameter</strong>. (reduce time cost)</li> </ol> <h2 id=8-visualizing-and-understanding>8 - Visualizing and Understanding<a class=headerlink href=#8-visualizing-and-understanding title="Permanent link">¶</a></h2> <h3 id=feature-visualization-and-inversion>Feature Visualization and Inversion<a class=headerlink href=#feature-visualization-and-inversion title="Permanent link">¶</a></h3> <h4 id=visualizing-what-models-have-learned>Visualizing what models have learned<a class=headerlink href=#visualizing-what-models-have-learned title="Permanent link">¶</a></h4> <table> <thead> <tr> <th>Visualize Areas</th> <th></th> </tr> </thead> <tbody> <tr> <td>Filters</td> <td>Visualize the raw weights of each convolution kernel. (better in the first layer)</td> </tr> <tr> <td>Final Layer Features</td> <td>Run dimensionality reduction for features in the last FC layer. (PCA, t-SNE...)</td> </tr> <tr> <td>Activations</td> <td>Visualize activated areas. (<a href=https://arxiv.org/abs/1506.06579>Understanding Neural Networks Through Deep Visualization</a>)</td> </tr> </tbody> </table> <h4 id=understanding-input-pixels>Understanding input pixels<a class=headerlink href=#understanding-input-pixels title="Permanent link">¶</a></h4> <h5 id=maximally-activating-patches>Maximally Activating Patches<a class=headerlink href=#maximally-activating-patches title="Permanent link">¶</a></h5> <ol> <li>Pick a layer and a channel.</li> <li>Run many images through the network, record values of the chosen channel.</li> <li>Visualize image patches that correspond to maximal activation features.</li> </ol> <p>For example, we have a layer with shape <span class=arithmatex>\(128\times13\times13\)</span>. We pick the 17th channel from all 128 channels. Then we run many pictures through the network. During each run we can find a maximal activation feature among all the <span class=arithmatex>\(13\times13\)</span> features in channel 17. We then record the corresponding picture patch for each maximal activation feature. At last, we visualize all picture patches for each feature.</p> <p>This will help us find the relationship between each maximal activation feature and its corresponding picture patches.</p> <p>(each row of the following picture represents a feature)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-activating_patches.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-activating_patches.png></a></p> <h5 id=saliency-via-occlusion>Saliency via Occlusion<a class=headerlink href=#saliency-via-occlusion title="Permanent link">¶</a></h5> <p>Mask part of the image before feeding to CNN, check how much predicted probabilities change.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_occlusion.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_occlusion.png></a></p> <h5 id=saliency-via-backprop>Saliency via Backprop<a class=headerlink href=#saliency-via-backprop title="Permanent link">¶</a></h5> <ol> <li>Compute gradient of (unnormalized) class score with respect to image pixels.</li> <li>Take absolute value and max over RGB channels to get <strong>saliency maps</strong>.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop_p.png></a></p> <h5 id=intermediate-features-via-guided-backprop>Intermediate Features via Guided Backprop<a class=headerlink href=#intermediate-features-via-guided-backprop title="Permanent link">¶</a></h5> <ol> <li>Pick a single intermediate neuron. (e.g. one feature in a <span class=arithmatex>\(128\times13\times13\)</span> feature map)</li> <li>Compute gradient of neuron value with respect to image pixels.</li> </ol> <p><a href=https://arxiv.org/abs/1412.6806>Striving for Simplicity: The All Convolutional Net</a></p> <p>Just like "Maximally Activating Patches", this could find the part of an image that a neuron responds to.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop_p.png></a></p> <h5 id=gradient-ascent>Gradient Ascent<a class=headerlink href=#gradient-ascent title="Permanent link">¶</a></h5> <p>Generate a synthetic image that maximally activates a neuron.</p> <ol> <li>Initialize image <span class=arithmatex>\(I\)</span> to zeros.</li> <li>Forward image to compute current scores <span class=arithmatex>\(S_c(I)\)</span> (for class <span class=arithmatex>\(c\)</span> before softmax).</li> <li>Backprop to get gradient of neuron value with respect to image pixels.</li> <li>Make a small update to the image.</li> </ol> <p>Objective: <span class=arithmatex>\(\max S_c(I)-\lambda\lVert I\lVert^2\)</span></p> <p><a href=https://arxiv.org/abs/1312.6034>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent_p.png></a></p> <h3 id=adversarial-examples>Adversarial Examples<a class=headerlink href=#adversarial-examples title="Permanent link">¶</a></h3> <p>Find an fooling image that can make the network misclassify correctly-classified images when it is added to the image.</p> <ol> <li>Start from an arbitrary image.</li> <li>Pick an arbitrary class.</li> <li>Modify the image to maximize the class.</li> <li>Repeat until network is fooled.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-adversarial_examples.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-adversarial_examples.png></a></p> <h3 id=deepdream-and-style-transfer>DeepDream and Style Transfer<a class=headerlink href=#deepdream-and-style-transfer title="Permanent link">¶</a></h3> <h4 id=feature-inversion>Feature Inversion<a class=headerlink href=#feature-inversion title="Permanent link">¶</a></h4> <p>Given a CNN feature vector <span class=arithmatex>\(\Phi_0\)</span> for an image, find a new image <span class=arithmatex>\(x\)</span> that:</p> <ol> <li>Features of new image <span class=arithmatex>\(\Phi(x)\)</span> matches the given feature vector <span class=arithmatex>\(\Phi_0\)</span>.</li> <li>"looks natural”. (image prior regularization)</li> </ol> <p>Objective: <span class=arithmatex>\(\min \lVert\Phi(x)-\Phi_0\lVert+\lambda R(x)\)</span></p> <p><a href=https://arxiv.org/abs/1412.0035>Understanding Deep Image Representations by Inverting Them</a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-feature_inversion.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-feature_inversion.png></a></p> <h4 id=deepdream-amplify-existing-features>DeepDream: Amplify Existing Features<a class=headerlink href=#deepdream-amplify-existing-features title="Permanent link">¶</a></h4> <p>Given an image, amplify the neuron activations at a layer to generate a new one.</p> <ol> <li>Forward: compute activations at chosen layer.</li> <li>Set gradient of chosen layer equal to its activation.</li> <li>Backward: Compute gradient on image.</li> <li>Update image.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-deepdream.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-deepdream.png></a></p> <h4 id=texture-synthesis>Texture Synthesis<a class=headerlink href=#texture-synthesis title="Permanent link">¶</a></h4> <h5 id=nearest-neighbor>Nearest Neighbor<a class=headerlink href=#nearest-neighbor title="Permanent link">¶</a></h5> <ol> <li>Generate pixels one at a time in scanline order</li> <li>Form neighborhood of already generated pixels, copy the nearest neighbor from input.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_nn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_nn.png></a></p> <h5 id=neural-texture-synthesis>Neural Texture Synthesis<a class=headerlink href=#neural-texture-synthesis title="Permanent link">¶</a></h5> <p><span>Gram Matrix:<span class=heti-spacing> </span></span><a href=https://zhuanlan.zhihu.com/p/187345192>格拉姆矩阵（Gram matrix）详细解读</a></p> <ol> <li>Pretrain a CNN on ImageNet.</li> <li>Run input texture forward through CNN, record activations on every layer.</li> </ol> <p>Layer <span class=arithmatex>\(i\)</span> gives feature map of shape <span class=arithmatex>\(C_i\times H_i\times W_i\)</span>.</p> <ol start=3> <li>At each layer compute the <strong>Gram matrix</strong> <span class=arithmatex>\(G_i\)</span> giving outer product of features.</li> </ol> <ul> <li>Reshape feature map at layer <span class=arithmatex>\(i\)</span> to <span class=arithmatex>\(C_i\times H_iW_i\)</span>.</li> <li>Compute the <strong>Gram matrix</strong> <span class=arithmatex>\(G_i\)</span> with shape <span class=arithmatex>\(C_i\times C_i\)</span>.</li> </ul> <ol start=4> <li>Initialize generated image from random noise.</li> <li>Pass generated image through CNN, compute <strong>Gram matrix</strong> <span class=arithmatex>\(\hat{G}_l\)</span> on each layer.</li> <li>Compute loss: Weighted sum of L2 distance between <strong>Gram matrices</strong>.</li> </ol> <ul> <li><span class=arithmatex>\(E_l=\frac{1}{aN_l^2M_l^2}\sum_{i,j}\Big(G_i^{(i,j)}-\hat{G}_i^{(i,j)}\Big)^2\\\)</span></li> <li><span class=arithmatex>\(\mathcal{L}(\vec{x},\hat{\vec{x}})=\sum_{l=0}^L\omega_lE_l\\\)</span></li> </ul> <ol start=7> <li>Backprop to get gradient on image.</li> <li>Make gradient step on image.</li> <li>GOTO 5.</li> </ol> <p><a href=https://arxiv.org/abs/1505.07376>Texture Synthesis Using Convolutional Neural Networks</a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural_p.png></a></p> <h4 id=style-transfer>Style Transfer<a class=headerlink href=#style-transfer title="Permanent link">¶</a></h4> <h5 id=feature-gram-reconstruction>Feature + Gram Reconstruction<a class=headerlink href=#feature-gram-reconstruction title="Permanent link">¶</a></h5> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_p.png></a></p> <p><strong>Problem:</strong> Style transfer requires many forward / backward passes. Very slow!</p> <h5 id=fast-style-transfer>Fast Style Transfer<a class=headerlink href=#fast-style-transfer title="Permanent link">¶</a></h5> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast_p.png></a></p> <h2 id=9-object-detection-and-image-segmentation>9 - Object Detection and Image Segmentation<a class=headerlink href=#9-object-detection-and-image-segmentation title="Permanent link">¶</a></h2> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-tasks.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-tasks.png></a></p> <h3 id=semantic-segmentation>Semantic Segmentation<a class=headerlink href=#semantic-segmentation title="Permanent link">¶</a></h3> <p><strong>Paired Training Data:</strong> For each training image, each pixel is labeled with a semantic category.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation.png></a></p> <p><strong>Fully Convolutional Network:</strong> Design a network with only convolutional layers without downsampling operators to make predictions for pixels all at once!</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv.png></a></p> <p><strong>Problem:</strong> Convolutions at original image resolution will be very expensive...</p> <p><strong>Solution:</strong> Design fully convolutional network with <strong>downsampling</strong> and <strong>upsampling</strong> inside it!</p> <ul> <li><strong>Downsampling:</strong> Pooling, strided convolution.</li> <li><strong>Upsampling:</strong> Unpooling, transposed convolution.</li> </ul> <p><strong>Unpooling:</strong></p> <table> <thead> <tr> <th style="text-align: center;">Nearest Neighbor</th> <th style="text-align: center;">"Bed of Nails"</th> <th style="text-align: center;">"Position Memory"</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_nn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_nn.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_bed_of_nails.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_bed_of_nails.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_memory.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_memory.png></a></td> </tr> </tbody> </table> <p><strong>Transposed Convolution:</strong> (example size <span class=arithmatex>\(3\times3\)</span>, stride <span class=arithmatex>\(2\)</span>, pad <span class=arithmatex>\(1\)</span>)</p> <table> <thead> <tr> <th style="text-align: center;">Normal Convolution</th> <th style="text-align: center;">Transposed Convolution</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution.png></a></td> </tr> <tr> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal_m.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal_m.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_m.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_m.png></a></td> </tr> </tbody> </table> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv_down.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv_down.png></a></p> <h3 id=object-detection>Object Detection<a class=headerlink href=#object-detection title="Permanent link">¶</a></h3> <h4 id=single-object>Single Object<a class=headerlink href=#single-object title="Permanent link">¶</a></h4> <p>Classification + Localization. (classification + regression problem)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-object_detection_single.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-object_detection_single.png></a></p> <h4 id=multiple-object>Multiple Object<a class=headerlink href=#multiple-object title="Permanent link">¶</a></h4> <h5 id=r-cnn>R-CNN<a class=headerlink href=#r-cnn title="Permanent link">¶</a></h5> <p>Using selective search to find “blobby” image regions that are likely to contain objects.</p> <ol> <li>Find regions of interest (RoI) using selective search. (region proposal)</li> <li>Forward each region through ConvNet.</li> <li>Classify features with SVMs.</li> </ol> <p><strong>Problem:</strong> Very slow. Need to do 2000 independent forward passes for each image!</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-rcnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-rcnn.png></a></p> <h5 id=fast-r-cnn>Fast R-CNN<a class=headerlink href=#fast-r-cnn title="Permanent link">¶</a></h5> <p>Pass the image through ConvNet before cropping. Crop the conv feature instead.</p> <ol> <li>Run whole image through ConvNet.</li> <li>Find regions of interest (RoI) from conv features using selective search. (<strong>region proposal</strong>)</li> <li>Classify RoIs using CNN.</li> </ol> <p><strong>Problem:</strong> Runtime is dominated by region proposals. (about <span class=arithmatex>\(90\%\)</span> time cost)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-fast_rcnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-fast_rcnn.png></a></p> <h5 id=faster-r-cnn>Faster R-CNN<a class=headerlink href=#faster-r-cnn title="Permanent link">¶</a></h5> <p>Insert Region Proposal Network (<strong>RPN</strong>) to predict proposals from features.</p> <p>Otherwise same as Fast R-CNN: Crop features for each proposal, classify each one.</p> <p><strong>Region Proposal Network (RPN) :</strong> Slide many fixed windows over ConvNet features.</p> <ol> <li>Treat each point in the feature map as the <strong>anchor</strong>. </li> </ol> <p>We have <span class=arithmatex>\(k\)</span> fixed windows (<strong>anchor boxes</strong>) of different size/scale centered with each anchor.</p> <ol start=2> <li>For each anchor box, predict whether it contains an object.</li> </ol> <p>For positive boxes, also predict a corrections to the ground-truth box.</p> <ol start=3> <li>Slide anchor over the feature map, get the <strong>“objectness” score</strong> for each box at each point.</li> <li>Sort the “objectness” score, take top <span class=arithmatex>\(300\)</span> as the proposals.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn_rpn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn_rpn.png></a></p> <p><strong>Faster R-CNN is a Two-stage object detector:</strong></p> <ol> <li>First stage: Run once per image</li> </ol> <p>Backbone network</p> <p>Region proposal network</p> <ol start=2> <li>Second stage: Run once per region</li> </ol> <p>Crop features: RoI pool / align</p> <p>Predict object class</p> <p>Prediction bbox offset</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn.png></a></p> <h5 id=single-stage-object-detectors-yolo>Single-Stage Object Detectors: YOLO<a class=headerlink href=#single-stage-object-detectors-yolo title="Permanent link">¶</a></h5> <p><a href=https://arxiv.org/abs/1506.02640>You Only Look Once: Unified, Real-Time Object Detection</a></p> <ol> <li>Divide image into grids. (example image grids shape <span class=arithmatex>\(7\times7\)</span>)</li> <li>Set anchors in the middle of each grid.</li> <li>For each grid: - Using <span class=arithmatex>\(B\)</span> anchor boxes to regress <span class=arithmatex>\(5\)</span> numbers: <span class=arithmatex>\(\text{dx, dy, dh, dw, confidence}\)</span>. - Predict scores for each of <span class=arithmatex>\(C\)</span> classes.</li> <li>Finally the output is <span class=arithmatex>\(7\times7\times(5B+C)\)</span>.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-yolo.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-yolo.png></a></p> <h3 id=instance-segmentation>Instance Segmentation<a class=headerlink href=#instance-segmentation title="Permanent link">¶</a></h3> <p><strong>Mask R-CNN:</strong> Add a small mask network that operates on each RoI and predicts a <span class=arithmatex>\(28\times28\)</span> binary mask.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn.png></a></p> <p>Mask R-CNN performs very good results!</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn_p.png></a></p> <h2 id=10-recurrent-neural-networks>10 - Recurrent Neural Networks<a class=headerlink href=#10-recurrent-neural-networks title="Permanent link">¶</a></h2> <p>Supplement content added according to <a href=https://www.deeplearningbook.org/contents/rnn.html>Deep Learning Book - RNN</a>.</p> <h3 id=recurrent-neural-network-rnn>Recurrent Neural Network (RNN)<a class=headerlink href=#recurrent-neural-network-rnn title="Permanent link">¶</a></h3> <h4 id=motivation-sequence-processing>Motivation: Sequence Processing<a class=headerlink href=#motivation-sequence-processing title="Permanent link">¶</a></h4> <table> <thead> <tr> <th style="text-align: center;">One to One</th> <th style="text-align: center;">One to Many</th> <th style="text-align: center;">Many to One</th> <th style="text-align: center;">Many to Many</th> <th style="text-align: center;">Many to Many</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_11.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_11.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_1m.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_1m.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_m1.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_m1.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm.png></a></td> <td style="text-align: center;"><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm_2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm_2.png></a></td> </tr> <tr> <td style="text-align: center;">Vanilla Neural Networks</td> <td style="text-align: center;">Image Captioning</td> <td style="text-align: center;">Action Prediction</td> <td style="text-align: center;">Video Captioning</td> <td style="text-align: center;">Video Classification on Frame Level</td> </tr> </tbody> </table> <h4 id=vanilla-rnn>Vanilla RNN<a class=headerlink href=#vanilla-rnn title="Permanent link">¶</a></h4> <p><span class=arithmatex>\(x^{(t)}\)</span> : Input at time <span class=arithmatex>\(t\)</span>.</p> <p><span class=arithmatex>\(h^{(t)}\)</span> : State at time <span class=arithmatex>\(t\)</span>.</p> <p><span class=arithmatex>\(o^{(t)}\)</span> : Output at time <span class=arithmatex>\(t\)</span>​​.</p> <p><span class=arithmatex>\(y^{(t)}\)</span> : Expected output at time <span class=arithmatex>\(t\)</span>.</p> <h5 id=many-to-one>Many to One<a class=headerlink href=#many-to-one title="Permanent link">¶</a></h5> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_m1.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_m1.png></a></p> <table> <thead> <tr> <th>Calculation</th> <th></th> </tr> </thead> <tbody> <tr> <td>State Transition</td> <td><span class=arithmatex>\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+b)\)</span></td> </tr> <tr> <td>Output Calculation</td> <td><span class=arithmatex>\(o^{(\tau)}=\text{sigmoid}\ \big(Vh^{(\tau)}+c\big)\)</span></td> </tr> </tbody> </table> <h5 id=many-to-many-type-2>Many to Many (type 2)<a class=headerlink href=#many-to-many-type-2 title="Permanent link">¶</a></h5> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_mm.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_mm.png></a></p> <table> <thead> <tr> <th>Calculation</th> <th></th> </tr> </thead> <tbody> <tr> <td>State Transition</td> <td><span class=arithmatex>\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+b)\)</span></td> </tr> <tr> <td>Output Calculation</td> <td><span class=arithmatex>\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td> </tr> </tbody> </table> <h4 id=rnn-with-teacher-forcing>RNN with Teacher Forcing<a class=headerlink href=#rnn-with-teacher-forcing title="Permanent link">¶</a></h4> <p>Update current state according to last-time <strong>output</strong> instead of last-time <strong>state</strong>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_tf.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_tf.png></a></p> <table> <thead> <tr> <th>Calculation</th> <th></th> </tr> </thead> <tbody> <tr> <td>State Transition</td> <td><span class=arithmatex>\(h^{(t)}=\tanh(Wo^{(t-1)}+Ux^{(t)}+b)\)</span></td> </tr> <tr> <td>Output Calculation</td> <td><span class=arithmatex>\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td> </tr> </tbody> </table> <h4 id=rnn-with-output-forwarding>RNN with "Output Forwarding"<a class=headerlink href=#rnn-with-output-forwarding title="Permanent link">¶</a></h4> <p>We can also combine last-state <strong>output</strong> with this-state <strong>input</strong> together.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_output.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_output.png></a></p> <table> <thead> <tr> <th>Calculation</th> <th></th> </tr> </thead> <tbody> <tr> <td>State Transition (training)</td> <td><span class=arithmatex>\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+Ry^{(t-1)}+b)\)</span></td> </tr> <tr> <td>State Transition (testing)</td> <td><span class=arithmatex>\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+Ro^{(t-1)}+b)\)</span></td> </tr> <tr> <td>Output Calculation</td> <td><span class=arithmatex>\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td> </tr> </tbody> </table> <p>Usually we use <span class=arithmatex>\(o^{(t-1)}\)</span> in place of <span class=arithmatex>\(y^{(t-1)}\)</span> at testing time.</p> <h4 id=bidirectional-rnn>Bidirectional RNN<a class=headerlink href=#bidirectional-rnn title="Permanent link">¶</a></h4> <p>When dealing with <strong>a whole input sequence</strong>, we can process features from two directions.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_bidirectional.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_bidirectional.png></a></p> <table> <thead> <tr> <th>Calculation</th> <th></th> </tr> </thead> <tbody> <tr> <td>State Transition (forward)</td> <td><span class=arithmatex>\(h^{(t)}=\tanh(W_1h^{(t-1)}+U_1x^{(t)}+b_1)\)</span></td> </tr> <tr> <td>State Transition (backward)</td> <td><span class=arithmatex>\(g^{(t)}=\tanh(W_2g^{(t+1)}+U_2x^{(t)}+b_2)\)</span></td> </tr> <tr> <td>Output Calculation</td> <td><span class=arithmatex>\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+Wg^{(t)}+c\big)\)</span></td> </tr> </tbody> </table> <h4 id=encoder-decoder-sequence-to-sequence-rnn>Encoder-Decoder Sequence to Sequence RNN<a class=headerlink href=#encoder-decoder-sequence-to-sequence-rnn title="Permanent link">¶</a></h4> <p>This is a <strong>many-to-many structure (type 1)</strong>.</p> <p>First we encode information according to <span class=arithmatex>\(x\)</span> with no output.</p> <p>Later we decode information according to <span class=arithmatex>\(y\)</span> with no input.</p> <p><span class=arithmatex>\(C\)</span> : Context vector, often <span class=arithmatex>\(C=h^{(T)}\)</span> (last state of encoder).</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_encoder.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_encoder.png></a></p> <table> <thead> <tr> <th>Calculation</th> <th></th> </tr> </thead> <tbody> <tr> <td>State Transition (encode)</td> <td><span class=arithmatex>\(h^{(t)}=\tanh(W_1h^{(t-1)}+U_1x^{(t)}+b_1)\)</span></td> </tr> <tr> <td>State Transition (decode, training)</td> <td><span class=arithmatex>\(s^{(t)}=\tanh(W_2s^{(t-1)}+U_2y^{(t)}+TC+b_2)\)</span></td> </tr> <tr> <td>State Transition (decode, testing)</td> <td><span class=arithmatex>\(s^{(t)}=\tanh(W_2s^{(t-1)}+U_2o^{(t)}+TC+b_2)\)</span></td> </tr> <tr> <td>Output Calculation</td> <td><span class=arithmatex>\(o^{(t)}=\text{sigmoid}\ \big(Vs^{(t)}+c\big)\)</span></td> </tr> </tbody> </table> <h4 id=example-image-captioning>Example: Image Captioning<a class=headerlink href=#example-image-captioning title="Permanent link">¶</a></h4> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_example.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_example.png></a></p> <h4 id=summary_2>Summary<a class=headerlink href=#summary_2 title="Permanent link">¶</a></h4> <p><strong>Advantages of RNN:</strong></p> <ol> <li>Can process any length input.</li> <li>Computation for step <span class=arithmatex>\(t\)</span> can (in theory) use information from many steps back.</li> <li>Model size doesn’t increase for longer input.</li> <li>Same weights applied on every timestep, so there is symmetry in how inputs are processed.</li> </ol> <p><strong>Disadvantages of RNN:</strong></p> <ol> <li>Recurrent computation is slow.</li> <li>In practice, difficult to access information from many steps back.</li> <li>Problems with gradient exploding and gradient vanishing. <strong>(check <a href=https://www.deeplearningbook.org/contents/rnn.html>Deep Learning Book - RNN</a> Page 396, Chap 10.7)</strong></li> </ol> <h3 id=long-short-term-memory-lstm>Long Short Term Memory (LSTM)<a class=headerlink href=#long-short-term-memory-lstm title="Permanent link">¶</a></h3> <p>Add a "cell block" to store history weights.</p> <p><span class=arithmatex>\(c^{(t)}\)</span> : Cell at time <span class=arithmatex>\(t\)</span>.</p> <p><span class=arithmatex>\(f^{(t)}\)</span> : <strong>Forget gate</strong> at time <span class=arithmatex>\(t\)</span>. Deciding whether to erase the cell.</p> <p><span class=arithmatex>\(i^{(t)}\)</span> : <strong>Input gate</strong> at time <span class=arithmatex>\(t\)</span>. Deciding whether to write to the cell.</p> <p><span class=arithmatex>\(g^{(t)}\)</span> : <strong>External input gate</strong> at time <span class=arithmatex>\(t\)</span>. Deciding how much to write to the cell.</p> <p><span class=arithmatex>\(o^{(t)}\)</span> : <strong>Output gate</strong> at time <span class=arithmatex>\(t\)</span>. Deciding how much to reveal the cell.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm.png></a></p> <table> <thead> <tr> <th>Calculation (Gate)</th> <th></th> </tr> </thead> <tbody> <tr> <td>Forget Gate</td> <td><span class=arithmatex>\(f^{(t)}=\text{sigmoid}\ \big(W_fh^{(t-1)}+U_fx^{(t)}+b_f\big)\)</span></td> </tr> <tr> <td>Input Gate</td> <td><span class=arithmatex>\(i^{(t)}=\text{sigmoid}\ \big(W_ih^{(t-1)}+U_ix^{(t)}+b_i\big)\)</span></td> </tr> <tr> <td>External Input Gate</td> <td><span class=arithmatex>\(g^{(t)}=\tanh(W_gh^{(t-1)}+U_gx^{(t)}+b_g)\)</span></td> </tr> <tr> <td>Output Gate</td> <td><span class=arithmatex>\(o^{(t)}=\text{sigmoid}\ \big(W_oh^{(t-1)}+U_ox^{(t)}+b_o\big)\)</span></td> </tr> </tbody> </table> <table> <thead> <tr> <th>Calculation (Main)</th> <th></th> </tr> </thead> <tbody> <tr> <td>Cell Transition</td> <td><span class=arithmatex>\(c^{(t)}=f^{(t)}\odot c^{(t-1)}+i^{(t)}\odot g^{(t)}\)</span></td> </tr> <tr> <td>State Transition</td> <td><span class=arithmatex>\(h^{(t)}=o^{(t)}\odot\tanh(c^{(t)})\)</span></td> </tr> <tr> <td>Output Calculation</td> <td><span class=arithmatex>\(O^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td> </tr> </tbody> </table> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm_gradient.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm_gradient.png></a></p> <h3 id=other-rnn-variants>Other RNN Variants<a class=headerlink href=#other-rnn-variants title="Permanent link">¶</a></h3> <p>GRU...</p> <h2 id=11-attention-and-transformers>11 - Attention and Transformers<a class=headerlink href=#11-attention-and-transformers title="Permanent link">¶</a></h2> <h3 id=rnn-with-attention>RNN with Attention<a class=headerlink href=#rnn-with-attention title="Permanent link">¶</a></h3> <p><strong>Encoder-Decoder Sequence to Sequence RNN Problem:</strong></p> <p>Input sequence bottlenecked through a fixed-sized context vector <span class=arithmatex>\(C\)</span>. (e.g. <span class=arithmatex>\(T=1000\)</span>)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_sequence.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_sequence.png></a></p> <p><strong>Intuitive Solution:</strong></p> <p>Generate new context vector <span class=arithmatex>\(C_t\)</span> at each step <span class=arithmatex>\(t\)</span> !</p> <p><span class=arithmatex>\(e_{t,i}\)</span> : Alignment score for input <span class=arithmatex>\(i\)</span> at state <span class=arithmatex>\(t\)</span>. <strong>(scalar)</strong></p> <p><span class=arithmatex>\(a_{t,i}\)</span> : Attention weight for input <span class=arithmatex>\(i\)</span> at state <span class=arithmatex>\(t\)</span>.</p> <p><span class=arithmatex>\(C_t\)</span> : Context vector at state <span class=arithmatex>\(t\)</span>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_1.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_1.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_2.png></a></p> <table> <thead> <tr> <th>Calculation</th> <th></th> </tr> </thead> <tbody> <tr> <td>Alignment Score</td> <td><span class=arithmatex>\(e_i^{(t)}=f(s^{(t-1)},h^{(i)})\)</span>.<br>Where <span class=arithmatex>\(f\)</span> is an MLP.</td> </tr> <tr> <td>Attention Weight</td> <td><span class=arithmatex>\(a_i^{(t)}=\text{softmax}\ (e_i^{(t)})\)</span>.<br>Softmax includes all <span class=arithmatex>\(e_i\)</span> at state <span class=arithmatex>\(t\)</span>.</td> </tr> <tr> <td>Context Vector</td> <td><span class=arithmatex>\(C^{(t)}=\sum_i a_i^{(t)}h^{(i)}\)</span></td> </tr> <tr> <td>Decoder State Transition</td> <td><span class=arithmatex>\(s^{(t)}=\tanh(Ws^{(t-1)}+Uy^{(t)}+TC^{(t)}+b)\)</span></td> </tr> </tbody> </table> <p><strong>Example on Image Captioning:</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example_2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example_2.png></a></p> <h3 id=general-attention-layer>General Attention Layer<a class=headerlink href=#general-attention-layer title="Permanent link">¶</a></h3> <p>Add linear transformations to the input vector before attention.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-general_attention.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-general_attention.png></a></p> <p><strong>Notice:</strong></p> <ol> <li>Number of queries <span class=arithmatex>\(q\)</span> is variant. (can be <strong>different</strong> from the number of keys <span class=arithmatex>\(k\)</span>)</li> <li>Number of outputs <span class=arithmatex>\(y\)</span> is equal to the number of queries <span class=arithmatex>\(q\)</span>.</li> </ol> <p>Each <span class=arithmatex>\(y\)</span> is a linear weighting of values <span class=arithmatex>\(v\)</span>.</p> <ol start=3> <li>Alignment <span class=arithmatex>\(e\)</span> is divided by <span class=arithmatex>\(\sqrt{D}\)</span> to avoid "explosion of softmax", where <span class=arithmatex>\(D\)</span> is the dimension of input feature.</li> </ol> <h3 id=self-attention-layer>Self-attention Layer<a class=headerlink href=#self-attention-layer title="Permanent link">¶</a></h3> <p>The query vectors <span class=arithmatex>\(q\)</span> are also generated from the inputs.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention.png></a></p> <p>In this way, the shape of <span class=arithmatex>\(y\)</span> is equal to the shape of <span class=arithmatex>\(x\)</span>.</p> <p><strong>Example with CNN:</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_example.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_example.png></a></p> <h3 id=positional-encoding>Positional Encoding<a class=headerlink href=#positional-encoding title="Permanent link">¶</a></h3> <p>Self-attention layer doesn’t care about the orders of the inputs!</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_problem.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_problem.png></a></p> <p>To encode ordered sequences like language or spatially ordered image features, we can add positional encoding to the inputs.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding.png></a></p> <p>We use a function <span class=arithmatex>\(P:R\rightarrow R^d\)</span> to process the <strong>position</strong> <span class=arithmatex>\(i\)</span> into a <strong>d-dimensional vector</strong> <span class=arithmatex>\(p_i=P(i)\)</span>.</p> <table> <thead> <tr> <th>Constraint Condition of <span class=arithmatex>\(P\)</span></th> <th></th> </tr> </thead> <tbody> <tr> <td>Uniqueness</td> <td><span class=arithmatex>\(P(i)\ne P(j)\)</span></td> </tr> <tr> <td>Equidistance</td> <td><span class=arithmatex>\(\lVert P(i+k)-P(i)\rVert^2=\lVert P(j+k)-P(j)\rVert^2\)</span></td> </tr> <tr> <td>Boundness</td> <td><span class=arithmatex>\(P(i)\in[a,b]\)</span></td> </tr> <tr> <td>Determinacy</td> <td><span class=arithmatex>\(P(i)\)</span> is always a static value. (function is not dynamic)</td> </tr> </tbody> </table> <p>We can either train a encoder model, or design a fixed function.</p> <p><strong>A Practical Positional Encoding Method:</strong> Using <span class=arithmatex>\(\sin\)</span> and <span class=arithmatex>\(\cos\)</span> with different frequency <span class=arithmatex>\(\omega\)</span> at different dimension.</p> <p><span class=arithmatex>\(P(t)=\begin{bmatrix}\sin(\omega_1,t)\\\cos(\omega_1,t)\\\\\sin(\omega_2,t)\\\cos(\omega_2,t)\\\vdots\\\sin(\omega_{\frac{d}{2}},t)\\\cos(\omega_{\frac{d}{2}},t)\end{bmatrix}\)</span>, where frequency <span class=arithmatex>\(\omega_k=\frac{1}{10000^{\frac{2k}{d}}}\\\)</span>. (wave length <span class=arithmatex>\(\lambda=\frac{1}{\omega}=10000^{\frac{2k}{d}}\\\)</span>)</p> <p><span class=arithmatex>\(P(t)=\begin{bmatrix}\sin(1/10000^{\frac{2}{d}},t)\\\cos(1/10000^{\frac{2}{d}},t)\\\\\sin(1/10000^{\frac{4}{d}},t)\\\cos(1/10000^{\frac{4}{d}},t)\\\vdots\\\sin(1/10000^1,t)\\\cos(1/10000^1,t)\end{bmatrix}\)</span>, after we substitute <span class=arithmatex>\(\omega_k\)</span> into the equation.</p> <p><span class=arithmatex>\(P(t)\)</span> is a vector with size <span class=arithmatex>\(d\)</span>, where <span class=arithmatex>\(d\)</span> is a hyperparameter to choose according to the length of input sequence.</p> <p>An intuition of this method is the binary encoding of numbers.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_intuition.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_intuition.png></a></p> <p><a href=https://www.bilibili.com/video/BV1E3411B7Bz><span>[lecture 11d]<span class=heti-spacing> </span></span>注意力和<span class=heti-skip><span class=heti-spacing> </span>transformer (positional encoding<span class=heti-spacing> </span></span>补充，代码实现，距离计算<span><span class=heti-spacing> </span>)</span></a></p> <p><strong>It is easy to prove that <span class=arithmatex>\(P(t)\)</span> satisfies "Equidistance":</strong> (set <span class=arithmatex>\(d=2\)</span> for example)</p> <p><span class=arithmatex>\(\begin{aligned}\lVert P(i+k)-P(i)\rVert^2&amp;=\big[\sin(\omega_1,i+k)-\sin(\omega_1,i)\big]^2+\big[\cos(\omega_1,i+k)-\cos(\omega_1,i)\big]^2\\&amp;=2-2\sin(\omega_1,i+k)\sin(\omega_1,i)-2\cos(\omega_1,i+k)\cos(\omega_1,i)\\&amp;=2-2\cos(\omega_1,k)\end{aligned}\)</span></p> <p>So the distance is not associated with <span class=arithmatex>\(i\)</span>, we have <span class=arithmatex>\(\lVert P(i+k)-P(i)\rVert^2=\lVert P(j+k)-P(j)\rVert^2\)</span>.</p> <p><strong>Visualization of <span class=arithmatex>\(P(t)\)</span> features:</strong> (set <span class=arithmatex>\(d=32\)</span>, <span class=arithmatex>\(x\)</span> axis represents the position of sequence)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_p.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_p.png></a></p> <h3 id=masked-self-attention-layer>Masked Self-attention Layer<a class=headerlink href=#masked-self-attention-layer title="Permanent link">¶</a></h3> <p>To prevent vectors from looking at future vectors, we manually set alignment scores to <span class=arithmatex>\(-\infty\)</span>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-masked_self_attention.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-masked_self_attention.png></a></p> <h3 id=multi-head-self-attention-layer>Multi-head Self-attention Layer<a class=headerlink href=#multi-head-self-attention-layer title="Permanent link">¶</a></h3> <p>Multiple self-attention heads in parallel.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-multihead_self_attention.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-multihead_self_attention.png></a></p> <h3 id=transformer>Transformer<a class=headerlink href=#transformer title="Permanent link">¶</a></h3> <p><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a></p> <h4 id=encoder-block>Encoder Block<a class=headerlink href=#encoder-block title="Permanent link">¶</a></h4> <p><strong>Inputs:</strong> Set of vectors <span class=arithmatex>\(z\)</span>. (in which <span class=arithmatex>\(z_i\)</span> can be a <strong>word</strong> in a sentence, or a <strong>pixel</strong> in a picture...)</p> <p><strong>Output:</strong> Set of context vectors <span class=arithmatex>\(c\)</span>. (encoded <strong>features</strong> of <span class=arithmatex>\(z\)</span>)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_encoder.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_encoder.png></a></p> <p>The number of blocks <span class=arithmatex>\(N=6\)</span> in original paper.</p> <p><strong>Notice:</strong></p> <ol> <li>Self-attention is the only interaction <strong>between vectors</strong> <span class=arithmatex>\(x_0,x_1,\dots,x_n\)</span>.</li> <li>Layer norm and MLP operate independently <strong>per vector</strong>.</li> <li>Highly scalable, highly parallelizable, but high memory usage.</li> </ol> <h4 id=decoder-block>Decoder Block<a class=headerlink href=#decoder-block title="Permanent link">¶</a></h4> <p><strong>Inputs:</strong> Set of vectors <span class=arithmatex>\(y\)</span>. (<span class=arithmatex>\(y_i\)</span> can be a <strong>word</strong> in a sentence, or a <strong>pixel</strong> in a picture...)</p> <p><strong>Inputs:</strong> Set of context vectors <span class=arithmatex>\(c\)</span>.</p> <p><strong>Output:</strong> Set of vectors <span class=arithmatex>\(y'\)</span>. (decoded result, <span class=arithmatex>\(y'_i=y_{i+1}\)</span> for the first <span class=arithmatex>\(n-1\)</span> number of <span class=arithmatex>\(y'\)</span>)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_decoder.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_decoder.png></a></p> <p>The number of blocks <span class=arithmatex>\(N=6\)</span> in original paper.</p> <p><strong>Notice:</strong></p> <ol> <li>Masked self-attention only interacts with <strong>past inputs</strong>.</li> <li>Multi-head attention block is <strong>NOT</strong> self-attention. It attends over encoder outputs.</li> <li>Highly scalable, highly parallelizable, but high memory usage. (same as encoder)</li> </ol> <p><strong>Why we need mask in decoder:</strong></p> <ol> <li>Needs for the special formation of output <span class=arithmatex>\(y'_i=y_{i+1}\)</span>.</li> <li>Needs for parallel computation.</li> </ol> <p><a href=https://zhuanlan.zhihu.com/p/166608727>举个例子讲下<span class=heti-skip><span class=heti-spacing> </span>transformer<span class=heti-spacing> </span></span>的输入输出细节及其他</a></p> <p><a href=https://blog.csdn.net/season77us/article/details/104144613>在测试或者预测时，<span>Transformer<span class=heti-spacing> </span></span>里<span class=heti-skip><span class=heti-spacing> </span>decoder<span class=heti-spacing> </span></span>为什么还需要<span><span class=heti-spacing> </span>seq mask</span>？</a></p> <h4 id=example-on-image-captioning-only-with-transformers>Example on Image Captioning (Only with Transformers)<a class=headerlink href=#example-on-image-captioning-only-with-transformers title="Permanent link">¶</a></h4> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_example.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_example.png></a></p> <h3 id=comparing-rnns-to-transformer>Comparing RNNs to Transformer<a class=headerlink href=#comparing-rnns-to-transformer title="Permanent link">¶</a></h3> <table> <thead> <tr> <th></th> <th>RNNs</th> <th>Transformer</th> </tr> </thead> <tbody> <tr> <td><strong>Pros</strong></td> <td>LSTMs work reasonably well for <strong>long sequences</strong>.</td> <td>1. Good at <strong>long sequences</strong>. Each attention calculation looks at all inputs.<br>2. Can operate over unordered sets or <strong>ordered sequences</strong> with positional encodings.<br>3. <strong>Parallel computation:</strong> All alignment and attention scores for all inputs can be done in parallel.</td> </tr> <tr> <td><strong>Cons</strong></td> <td>1. Expects an <strong>ordered sequences</strong> of inputs.<br>2. <strong>Sequential computation:</strong> Subsequent hidden states can only be computed after the previous ones are done.</td> <td><strong>Requires a lot of memory:</strong> <span class=arithmatex>\(N\times M\)</span> alignment and attention scalers need to be calculated and stored for a single self-attention head.</td> </tr> </tbody> </table> <h3 id=comparing-convnets-to-transformer>Comparing ConvNets to Transformer<a class=headerlink href=#comparing-convnets-to-transformer title="Permanent link">¶</a></h3> <p>ConvNets strike back!</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_compare.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_compare.png></a></p> <h2 id=12-video-understanding>12 - Video Understanding<a class=headerlink href=#12-video-understanding title="Permanent link">¶</a></h2> <h3 id=video-classification>Video Classification<a class=headerlink href=#video-classification title="Permanent link">¶</a></h3> <p>Take video classification task for example.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification.png></a></p> <p>Input size: <span class=arithmatex>\(C\times T\times H\times W\)</span>.</p> <p>The problem is, videos are quite big. We can't afford to train on raw videos, instead we train on video clips.</p> <table> <thead> <tr> <th>Raw Videos</th> <th>Video Clips</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(1920\times1080,\ 30\text{fps}\)</span></td> <td><span class=arithmatex>\(112\times112,\ 5\text{f}/3.2\text{s}\)</span></td> </tr> <tr> <td><span class=arithmatex>\(10\text{GB}/\text{min}\)</span></td> <td><span class=arithmatex>\(588\text{KB}/\text{min}\)</span></td> </tr> </tbody> </table> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification_clips.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification_clips.png></a></p> <h3 id=plain-cnn-structure>Plain CNN Structure<a class=headerlink href=#plain-cnn-structure title="Permanent link">¶</a></h3> <h4 id=single-frame-2d-cnn>Single Frame 2D-CNN<a class=headerlink href=#single-frame-2d-cnn title="Permanent link">¶</a></h4> <p>Train a normal 2D-CNN model.</p> <p>Classify each frame independently.</p> <p>Average the result of each frame as the final result.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-single_frame_cnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-single_frame_cnn.png></a></p> <h4 id=late-fusion>Late Fusion<a class=headerlink href=#late-fusion title="Permanent link">¶</a></h4> <p>Get high-level appearance of each frame, and combine them.</p> <p>Run 2D-CNN on each frame, pool features and feed to Linear Layers.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion.png></a></p> <p><strong>Problem:</strong> Hard to compare low-level motion between frames.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion_problem.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion_problem.png></a></p> <h4 id=early-fusion>Early Fusion<a class=headerlink href=#early-fusion title="Permanent link">¶</a></h4> <p>Compare frames with very first Conv Layer, after that normal 2D-CNN.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-early_fusion.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-early_fusion.png></a></p> <p><strong>Problem:</strong> One layer of temporal processing may not be enough!</p> <h4 id=3d-cnn>3D-CNN<a class=headerlink href=#3d-cnn title="Permanent link">¶</a></h4> <p><strong>Convolve on 3 dimensions:</strong> Height, Width, Time.</p> <p><strong>Input size:</strong> <span class=arithmatex>\(C_{in}\times T\times H\times W\)</span>.</p> <p><strong>Kernel size:</strong> <span class=arithmatex>\(C_{in}\times C_{out}\times 3\times 3\times 3\)</span>.</p> <p><strong>Output size:</strong> <span class=arithmatex>\(C_{out}\times T\times H\times W\)</span>. (with zero paddling)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-3d_cnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-3d_cnn.png></a></p> <h4 id=c3d-vgg-of-3d-cnns>C3D (VGG of 3D-CNNs)<a class=headerlink href=#c3d-vgg-of-3d-cnns title="Permanent link">¶</a></h4> <p>The cost is quite expensive...</p> <table> <thead> <tr> <th>Network</th> <th>Calculation</th> </tr> </thead> <tbody> <tr> <td>AlexNet</td> <td>0.7 GFLOP</td> </tr> <tr> <td>VGG-16</td> <td>13.6 GFLOP</td> </tr> <tr> <td>C3D</td> <td><strong>39.5</strong> GFLOP</td> </tr> </tbody> </table> <h4 id=two-stream-networks>Two-Stream Networks<a class=headerlink href=#two-stream-networks title="Permanent link">¶</a></h4> <p>Separate motion and appearance.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-two_stream_flow.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-two_stream_flow.png></a></p> <h4 id=i3d-inflating-2d-networks-to-3d>I3D (Inflating 2D Networks to 3D)<a class=headerlink href=#i3d-inflating-2d-networks-to-3d title="Permanent link">¶</a></h4> <p>Take a 2D-CNN architecture.</p> <p>Replace each 2D conv/pool layer with a 3D version.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-i3d.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-i3d.png></a></p> <h3 id=modeling-long-term-temporal-structure>Modeling Long-term Temporal Structure<a class=headerlink href=#modeling-long-term-temporal-structure title="Permanent link">¶</a></h3> <h4 id=recurrent-convolutional-network>Recurrent Convolutional Network<a class=headerlink href=#recurrent-convolutional-network title="Permanent link">¶</a></h4> <p>Similar to multi-layer RNN, we replace the <strong>dot-product</strong> operation with <strong>convolution</strong>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn.png></a></p> <p>Feature size in layer <span class=arithmatex>\(L\)</span>, time <span class=arithmatex>\(t-1\)</span>: <span class=arithmatex>\(W_h\times H\times W\)</span>.</p> <p>Feature size in layer <span class=arithmatex>\(L-1\)</span>, time <span class=arithmatex>\(t\)</span>: <span class=arithmatex>\(W_x\times H\times W\)</span>.</p> <p>Feature size in layer <span class=arithmatex>\(L\)</span>, time <span class=arithmatex>\(t\)</span>: <span class=arithmatex>\((W_h+W_x)\times H\times W\)</span>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn_inside.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn_inside.png></a></p> <p><strong>Problem:</strong> RNNs are slow for long sequences. (can’t be parallelized)</p> <h4 id=spatio-temporal-self-attention>Spatio-temporal Self-attention<a class=headerlink href=#spatio-temporal-self-attention title="Permanent link">¶</a></h4> <p>Introduce self-attention into video classification problems.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention_net.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention_net.png></a></p> <h4 id=vision-transformers-for-video>Vision Transformers for Video<a class=headerlink href=#vision-transformers-for-video title="Permanent link">¶</a></h4> <p>Factorized attention: Attend over space / time.</p> <p>So many papers...</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-vision_transformer.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-vision_transformer.png></a></p> <h3 id=visualizing-video-models>Visualizing Video Models<a class=headerlink href=#visualizing-video-models title="Permanent link">¶</a></h3> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing_2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing_2.png></a></p> <h3 id=multimodal-video-understanding>Multimodal Video Understanding<a class=headerlink href=#multimodal-video-understanding title="Permanent link">¶</a></h3> <h4 id=temporal-action-localization>Temporal Action Localization<a class=headerlink href=#temporal-action-localization title="Permanent link">¶</a></h4> <p>Given a long untrimmed video sequence, identify frames corresponding to different actions.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_temporal_localization.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_temporal_localization.png></a></p> <h4 id=spatio-temporal-detection>Spatio-Temporal Detection<a class=headerlink href=#spatio-temporal-detection title="Permanent link">¶</a></h4> <p>Given a long untrimmed video, detect all the people in both space and time and classify the activities they are performing.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_s_t_detection.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_s_t_detection.png></a></p> <h4 id=visually-guided-audio-source-separation>Visually-guided Audio Source Separation<a class=headerlink href=#visually-guided-audio-source-separation title="Permanent link">¶</a></h4> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_voice_separation.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_voice_separation.png></a></p> <p>And So on...</p> <h2 id=13-generative-models>13 - Generative Models<a class=headerlink href=#13-generative-models title="Permanent link">¶</a></h2> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-generative_model.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-generative_model.png></a></p> <h3 id=pixelrnn-and-pixelcnn>PixelRNN and PixelCNN<a class=headerlink href=#pixelrnn-and-pixelcnn title="Permanent link">¶</a></h3> <h4 id=fully-visible-belief-network-fvbn>Fully Visible Belief Network (FVBN)<a class=headerlink href=#fully-visible-belief-network-fvbn title="Permanent link">¶</a></h4> <p><span class=arithmatex>\(p(x)\)</span> : Likelihood of image <span class=arithmatex>\(x\)</span>.</p> <p><span class=arithmatex>\(p(x_1,x_2,\dots,x_n)\)</span> : Joint likelihood of all <span class=arithmatex>\(n\)</span> pixels in image <span class=arithmatex>\(x\)</span>.</p> <p><span class=arithmatex>\(p(x_i|x_1,x_2,\dots,x_{i-1})\)</span> : Probability of pixel <span class=arithmatex>\(i\)</span> value given all previous pixels.</p> <p>For explicit density models, we have <span class=arithmatex>\(p(x)=p(x_1,x_2,\dots,x_n)=\prod_{i=1}^np(x_i|x_1,x_2,\dots,x_{i-1})\\\)</span>.</p> <p><strong>Objective:</strong> Maximize the likelihood of training data.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-likelihood.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-likelihood.png></a></p> <h4 id=pixelrnn>PixelRNN<a class=headerlink href=#pixelrnn title="Permanent link">¶</a></h4> <p>Generate image pixels starting from corner.</p> <p>Dependency on previous pixels modeled using an RNN (LSTM).</p> <p><strong>Drawback:</strong> Sequential generation is slow in both training and inference!</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_rnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_rnn.png></a></p> <h4 id=pixelcnn>PixelCNN<a class=headerlink href=#pixelcnn title="Permanent link">¶</a></h4> <p>Still generate image pixels starting from corner.</p> <p>Dependency on previous pixels modeled using a CNN over context region (masked convolution).</p> <p><strong>Drawback:</strong> Though its training is faster, its generation is still slow. <strong>(pixel by pixel)</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_cnn.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_cnn.png></a></p> <h3 id=variational-autoencoder>Variational Autoencoder<a class=headerlink href=#variational-autoencoder title="Permanent link">¶</a></h3> <p>Supplement content added according to <a href=https://arxiv.org/abs/1606.05908>Tutorial on Variational Autoencoders</a>. (<strong>paper with notes:</strong> <a href="..\Variational Autoencoder\papes\VAE Tutorial.pdf">VAE Tutorial.pdf</a>)</p> <p><a href=https://zhuanlan.zhihu.com/p/34998569>变分自编码器<span><span class=heti-spacing> </span>VAE</span>：原来是这么一回事<span class=heti-skip><span class=heti-spacing> </span>|<span class=heti-spacing> </span></span>附开源代码</a></p> <h4 id=autoencoder>Autoencoder<a class=headerlink href=#autoencoder title="Permanent link">¶</a></h4> <p>Learn a lower-dimensional feature representation with unsupervised approaches.</p> <p><span class=arithmatex>\(x\rightarrow z\)</span> : Dimension reduction for input features.</p> <p><span class=arithmatex>\(z\rightarrow \hat{x}\)</span> : Reconstruct input features.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder.png></a></p> <p>After training, we throw the decoder away and use the encoder for transferring.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder_transfer.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder_transfer.png></a></p> <p><strong>For generative models, there is a problem:</strong></p> <p>We can’t generate new images from an autoencoder because we don’t know the space of <span class=arithmatex>\(z\)</span>.</p> <h4 id=variational-autoencoder_1>Variational Autoencoder<a class=headerlink href=#variational-autoencoder_1 title="Permanent link">¶</a></h4> <h5 id=character-description>Character Description<a class=headerlink href=#character-description title="Permanent link">¶</a></h5> <p><span class=arithmatex>\(X\)</span> : Images. <strong>(random variable)</strong></p> <p><span class=arithmatex>\(Z\)</span> : Latent representations. <strong>(random variable)</strong></p> <p><span class=arithmatex>\(P(X)\)</span> : True distribution of all training images <span class=arithmatex>\(X\)</span>.</p> <p><span class=arithmatex>\(P(Z)\)</span> : True distribution of all latent representations <span class=arithmatex>\(Z\)</span>.</p> <p><span class=arithmatex>\(P(X|Z)\)</span> : True <strong>posterior</strong> distribution of all images <span class=arithmatex>\(X\)</span> with condition <span class=arithmatex>\(Z\)</span>.</p> <p><span class=arithmatex>\(P(Z|X)\)</span> : True <strong>prior</strong> distribution of all latent representations <span class=arithmatex>\(Z\)</span> with condition <span class=arithmatex>\(X\)</span>.</p> <p><span class=arithmatex>\(Q(Z|X)\)</span> : Approximated <strong>prior</strong> distribution of all latent representations <span class=arithmatex>\(Z\)</span> with condition <span class=arithmatex>\(X\)</span>.</p> <p><span class=arithmatex>\(x\)</span> : A specific image.</p> <p><span class=arithmatex>\(z\)</span> : A specific latent representation.</p> <p><span class=arithmatex>\(\theta\)</span>: Learned parameters in decoder network.</p> <p><span class=arithmatex>\(\phi\)</span>: Learned parameters in encoder network.</p> <p><span class=arithmatex>\(p_\theta(x)\)</span> : Probability that <span class=arithmatex>\(x\sim P(X)\)</span>.</p> <p><span class=arithmatex>\(p_\theta(z)\)</span> : Probability that <span class=arithmatex>\(z\sim P(Z)\)</span>.</p> <p><span class=arithmatex>\(p_\theta(x|z)\)</span> : Probability that <span class=arithmatex>\(x\sim P(X|Z)\)</span>.</p> <p><span class=arithmatex>\(p_\theta(z|x)\)</span> : Probability that <span class=arithmatex>\(z\sim P(Z|X)\)</span>.</p> <p><span class=arithmatex>\(q_\phi(z|x)\)</span> : Probability that <span class=arithmatex>\(z\sim Q(Z|X)\)</span>.</p> <h5 id=decoder>Decoder<a class=headerlink href=#decoder title="Permanent link">¶</a></h5> <p><strong>Objective:</strong></p> <p>Generate new images from <span class=arithmatex>\(\mathscr{z}\)</span>.</p> <ol> <li>Generate a value <span class=arithmatex>\(z^{(i)}\)</span> from the prior distribution <span class=arithmatex>\(P(Z)\)</span>.</li> <li>Generate a value <span class=arithmatex>\(x^{(i)}\)</span> from the conditional distribution <span class=arithmatex>\(P(X|Z)\)</span>.</li> </ol> <p><strong>Lemma:</strong></p> <p>Any distribution in <span class=arithmatex>\(d\)</span> dimensions can be generated by taking a set of <span class=arithmatex>\(d\)</span> variables that are <strong>normally distributed</strong> and mapping them through a sufficiently complicated function. (source: <a href=https://arxiv.org/abs/1606.05908>Tutorial on Variational Autoencoders</a>, Page 6)</p> <p><strong>Solutions:</strong></p> <ol> <li>Choose prior distribution <span class=arithmatex>\(P(Z)\)</span> to be a simple distribution, for example <span class=arithmatex>\(P(Z)\sim N(0,1)\)</span>.</li> <li>Learn the conditional distribution <span class=arithmatex>\(P(X|Z)\)</span> through a neural network (decoder) with parameter <span class=arithmatex>\(\theta\)</span>. </li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_decoder.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_decoder.png></a></p> <h5 id=encoder>Encoder<a class=headerlink href=#encoder title="Permanent link">¶</a></h5> <p><strong>Objective:</strong></p> <p>Learn <span class=arithmatex>\(\mathscr{z}\)</span> with training images.</p> <p><strong>Given:</strong> (From the decoder, we can deduce the following probabilities.)</p> <ol> <li><em>data likelihood:</em> <span class=arithmatex>\(p_\theta(x)=\int p_\theta(x|z)p_\theta(z)dz\)</span>.</li> <li><em>posterior density:</em> <span class=arithmatex>\(p_\theta(z|x)=\frac{p_\theta(x|z)p_\theta(z)}{p_\theta(x)}=\frac{p_\theta(x|z)p_\theta(z)}{\int p_\theta(x|z)p_\theta(z)dz}\)</span>.</li> </ol> <p><strong>Problem:</strong></p> <p>Both <span class=arithmatex>\(p_\theta(x)\)</span> and <span class=arithmatex>\(p_\theta(z|x)\)</span> are intractable. (can't be optimized directly as they contain <em>integral operation</em>)</p> <p><strong>Solution:</strong></p> <p>Learn <span class=arithmatex>\(Q(Z|X)\)</span> to approximate the true posterior <span class=arithmatex>\(P(Z|X)\)</span>.</p> <p>Use <span class=arithmatex>\(q_\phi(z|x)\)</span> in place of <span class=arithmatex>\(p_\theta(z|x)\)</span>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_encoder.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_encoder.png></a></p> <h5 id=variational-autoencoder-combination-of-encoder-and-decoder>Variational Autoencoder (Combination of Encoder and Decoder)<a class=headerlink href=#variational-autoencoder-combination-of-encoder-and-decoder title="Permanent link">¶</a></h5> <p><strong>Objective:</strong></p> <p>Maximize <span class=arithmatex>\(p_\theta(x)\)</span> for all <span class=arithmatex>\(x^{(i)}\)</span> in the training set.</p> <p>$$ \begin{aligned} \log p_\theta\big(x^{(i)}\big)&amp;=\mathbb{E}<em>{z\sim q</em>\phi\big(z|x^{(i)}\big)}\Big[\log p_\theta\big(x^{(i)}\big)\Big]\</p> <p>&amp;=\mathbb{E}<em>z\Bigg[\log\frac{p</em>\theta\big(x^{(i)}|z\big)p_\theta\big(z\big)}{p_\theta\big(z|x^{(i)}\big)}\Bigg]\quad\text{(Bayes' Rule)}\</p> <p>&amp;=\mathbb{E}<em>z\Bigg[\log\frac{p</em>\theta\big(x^{(i)}|z\big)p_\theta\big(z\big)}{p_\theta\big(z|x^{(i)}\big)}\frac{q_\phi\big(z|x^{(i)}\big)}{q_\phi\big(z|x^{(i)}\big)}\Bigg]\quad\text{(Multiply by Constant)}\</p> <p>&amp;=\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-\mathbb{E}<em>z\Bigg[\log\frac{q</em>\phi\big(z|x^{(i)}\big)}{p_\theta\big(z\big)}\Bigg]+\mathbb{E}<em>z\Bigg[\log\frac{p</em>\theta\big(z|x^{(i)}\big)}{q_\phi\big(z|x^{(i)}\big)}\Bigg]\quad\text{(Logarithm)}\</p> <p>&amp;=\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]+D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\quad\text{(KL Divergence)} \end{aligned} $$</p> <p><strong>Analyze the Formula by Term:</strong></p> <p><span class=arithmatex>\(\mathbb{E}_z\Big[\log p_\theta\big(x^{(i)}|z\big)\Big]\)</span>: Decoder network gives <span class=arithmatex>\(p_\theta\big(x^{(i)}|z\big)\)</span>, can compute estimate of this term through sampling.</p> <p><span class=arithmatex>\(D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]\)</span>: This KL term (between Gaussians for encoder and <span class=arithmatex>\(z\)</span> prior) has nice closed-form solution!</p> <p><span class=arithmatex>\(D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\)</span>: The part <span class=arithmatex>\(p_\theta\big(z|x^{(i)}\big)\)</span> is intractable. <strong>However, we know KL divergence always <span class=arithmatex>\(\ge0\)</span>.</strong></p> <p><strong>Tractable Lower Bound:</strong></p> <p>We can maximize the lower bound of that formula.</p> <p>As <span class=arithmatex>\(D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\ge0\)</span> , we can deduce that:</p> <p>$$ \begin{aligned} \log p_\theta\big(x^{(i)}\big)&amp;=\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]+D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\</p> <p>&amp;\ge\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big] \end{aligned} $$</p> <p>So the loss function <span class=arithmatex>\(\mathcal{L}\big(x^{(i)},\theta,\phi\big)=-\mathbb{E}_z\Big[\log p_\theta\big(x^{(i)}|z\big)\Big]+D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]\)</span>.</p> <p><span class=arithmatex>\(\mathbb{E}_z\Big[\log p_\theta\big(x^{(i)}|z\big)\Big]\)</span>: <strong><em>Decoder</em></strong>, reconstruct the input data. </p> <p><span class=arithmatex>\(D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]\)</span>: <strong><em>Encoder</em></strong>, make approximate posterior distribution close to prior.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_combination.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_combination.png></a></p> <h3 id=generative-adversarial-networks-gans>Generative Adversarial Networks (GANs)<a class=headerlink href=#generative-adversarial-networks-gans title="Permanent link">¶</a></h3> <h4 id=motivation-modeling>Motivation &amp; Modeling<a class=headerlink href=#motivation-modeling title="Permanent link">¶</a></h4> <p><strong>Objective:</strong> Not modeling any explicit density function.</p> <p><strong>Problem:</strong> Want to sample from complex, high-dimensional training distribution. <strong>No direct way to do this!</strong></p> <p><strong>Solution:</strong> Sample from a simple distribution, e.g. <strong>random noise</strong>. Learn the transformation to training distribution.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage1.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage1.png></a></p> <p><strong>Problem:</strong> We can't learn the <strong>mapping relation</strong> between sample <span class=arithmatex>\(z\)</span> and training images.</p> <p><strong>Solution:</strong> Use a <strong>discriminator network</strong> to tell whether the generate image is within data distribution or not.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage2.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage2.png></a></p> <p><strong>Discriminator network:</strong> Try to distinguish between real and fake images.</p> <p><strong>Generator network:</strong> Try to fool the discriminator by generating real-looking images.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage3.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage3.png></a></p> <p><span class=arithmatex>\(x\)</span> : Real data.</p> <p><span class=arithmatex>\(y\)</span> : Fake data, which is generated by the generator network. <span class=arithmatex>\(y=G_{\theta_g}(z)\)</span>.</p> <p><span class=arithmatex>\(D_{\theta_d}(x)\)</span> : Discriminator score, which is the likelihood of real image. <span class=arithmatex>\(D_{\theta_d}(x)\in[0,1]\)</span>.</p> <p><strong>Objective of discriminator network:</strong></p> <p><span class=arithmatex>\(\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p> <p><strong>Objective of generator network:</strong></p> <p><span class=arithmatex>\(\min_{\theta_g}\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p> <h4 id=training-strategy>Training Strategy<a class=headerlink href=#training-strategy title="Permanent link">¶</a></h4> <p>Two combine this two networks together, we can train them alternately:</p> <ol> <li>Gradient <strong>ascent</strong> on discriminator.</li> </ol> <p><span class=arithmatex>\(\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p> <ol start=2> <li>Gradient <strong>descent</strong> on generator.</li> </ol> <p><span class=arithmatex>\(\min_{\theta_g}\bigg[\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p> <p>However, the gradient of generator decreases with the value itself, making it <strong>hard to optimize</strong>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_gradient.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_gradient.png></a></p> <p>So we replace <span class=arithmatex>\(\log\big(1-D_{\theta_d}(y)\big)\)</span> with <span class=arithmatex>\(-\log D_{\theta_d}(y)\)</span>, and use gradient ascent instead.</p> <ol> <li>Gradient <strong>ascent</strong> on discriminator.</li> </ol> <p><span class=arithmatex>\(\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p> <ol start=2> <li>Gradient <strong>ascent</strong> on generator.</li> </ol> <p><span class=arithmatex>\(\max_{\theta_g}\bigg[\mathbb{E}_{z\sim p(z)}\Big(\log D_{\theta_d}(y)\Big)\bigg]\)</span></p> <h4 id=summary_3>Summary<a class=headerlink href=#summary_3 title="Permanent link">¶</a></h4> <p><strong>Pros:</strong> Beautiful, state-of-the-art samples!</p> <p><strong>Cons:</strong> </p> <ol> <li>Trickier / more unstable to train.</li> <li>Can’t solve inference queries such as <span class=arithmatex>\(p(x), p(z|x)\)</span>.</li> </ol> <h2 id=14-self-supervised-learning>14 - Self-supervised Learning<a class=headerlink href=#14-self-supervised-learning title="Permanent link">¶</a></h2> <p><strong>Aim:</strong> Solve “pretext” tasks that produce good features for downstream tasks.</p> <p><strong>Application:</strong></p> <ol> <li>Learn a feature extractor from pretext tasks. <strong>(self-supervised)</strong></li> <li>Attach a shallow network on the feature extractor.</li> <li>Train the shallow network on target task with small amount of labeled data. <strong>(supervised)</strong></li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-self_supervised_learning.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-self_supervised_learning.png></a></p> <h3 id=pretext-tasks>Pretext Tasks<a class=headerlink href=#pretext-tasks title="Permanent link">¶</a></h3> <p>Labels are generated automatically.</p> <h4 id=rotation>Rotation<a class=headerlink href=#rotation title="Permanent link">¶</a></h4> <p>Train a classifier on randomly rotated images.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rotation.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rotation.png></a></p> <h4 id=rearrangement>Rearrangement<a class=headerlink href=#rearrangement title="Permanent link">¶</a></h4> <p>Train a classifier on randomly shuffled image pieces.</p> <p>Predict the location of image pieces.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rearrangement.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rearrangement.png></a></p> <h4 id=inpainting>Inpainting<a class=headerlink href=#inpainting title="Permanent link">¶</a></h4> <p>Mask part of the image, train a network to predict the masked area.</p> <p>Method referencing <a href=https://arxiv.org/pdf/1604.07379.pdf>Context Encoders: Feature Learning by Inpainting</a>.</p> <p>Combine two types of loss together to get better performance:</p> <ol> <li><strong>Reconstruction loss (L2 loss):</strong> Used for reconstructing global features.</li> <li><strong>Adversarial loss:</strong> Used for generating texture features.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_inpainting.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_inpainting.png></a></p> <h4 id=coloring>Coloring<a class=headerlink href=#coloring title="Permanent link">¶</a></h4> <p>Transfer between greyscale images and colored images.</p> <p><strong>Cross-channel predictions for images:</strong> <a href=https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.pdf>Split-Brain Autoencoders</a>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_sb_ae.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_sb_ae.png></a></p> <p><strong>Video coloring:</strong> Establish mappings between reference and target frames in a learned feature space. <a href=https://arxiv.org/abs/1806.09594>Tracking Emerges by Colorizing Videos</a>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_video.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_video.png></a></p> <h4 id=summary-for-pretext-tasks>Summary for Pretext Tasks<a class=headerlink href=#summary-for-pretext-tasks title="Permanent link">¶</a></h4> <ol> <li>Pretext tasks focus on <strong>“visual common sense”</strong>.</li> <li>The models are forced learn good features about natural images.</li> <li>We <strong>don’t</strong> care about the performance of these <strong>pretext tasks</strong>. </li> </ol> <p>What we care is the performance of <strong>downstream tasks</strong>.</p> <h4 id=problems-of-specific-pretext-tasks>Problems of Specific Pretext Tasks<a class=headerlink href=#problems-of-specific-pretext-tasks title="Permanent link">¶</a></h4> <ol> <li>Coming up with <strong>individual</strong> pretext tasks is tedious.</li> <li>The learned representations may <strong>not be general</strong>.</li> </ol> <p><strong>Intuitive Solution:</strong> Contrastive Learning.</p> <h3 id=contrastive-representation-learning>Contrastive Representation Learning<a class=headerlink href=#contrastive-representation-learning title="Permanent link">¶</a></h3> <p><strong>Local additional references:</strong> <a href="....\DL\Contrastive Learning\Contrastive Learning.md">Contrastive Learning.md</a>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive.png></a></p> <p><strong>Objective:</strong></p> <p>Given a chosen score function <span class=arithmatex>\(s\)</span>, we aim to learn an encoder function <span class=arithmatex>\(f\)</span> that yields:</p> <ol> <li>For each sample <span class=arithmatex>\(x\)</span>, increase the similarity <span class=arithmatex>\(s\big(f(x),f(x^+)\big)\)</span> between <span class=arithmatex>\(x\)</span> and positive samples <span class=arithmatex>\(x^+\)</span>.</li> <li>Finally we want <span class=arithmatex>\(s\big(f(x),f(x^+)\big)\gg s\big(f(x),f(x^-)\big)\)</span>.</li> </ol> <p><strong>Loss Function:</strong> </p> <p>Given <span class=arithmatex>\(1\)</span> positive sample and <span class=arithmatex>\(N-1\)</span> negative samples:</p> <table> <thead> <tr> <th>InfoNCE Loss</th> <th>Cross Entropy Loss</th> </tr> </thead> <tbody> <tr> <td><span class=arithmatex>\(\begin{aligned}\mathcal{L}=-\mathbb{E}_X\Bigg[\log\frac{\exp{s\big(f(x),f(x^+)\big)}}{\exp{s\big(f(x),f(x^+)\big)}+\sum_{j=1}^{N-1}\exp{s\big(f(x),f(x^+)\big)}}\Bigg]\\\end{aligned}\)</span></td> <td><span class=arithmatex>\(\begin{aligned}\mathcal{L}&amp;=-\sum_{i=1}^Np(x_i)\log q(x_i)\\&amp;=-\mathbb{E}_X\big[\log q(x)\big]\\&amp;=-\mathbb{E}_X\Bigg[\log\frac{\exp(x)}{\sum_{j=1}^N\exp(x_j)}\Bigg]\end{aligned}\)</span></td> </tr> </tbody> </table> <p>The <em>InfoNCE Loss</em> is a lower bound on the <em>mutual information</em> between <span class=arithmatex>\(f(x)\)</span> and <span class=arithmatex>\(f(x^+)\)</span>:</p> <p><span class=arithmatex>\(\text{MI}\big[f(x),f(x^+)\big]\ge\log(N)-\mathcal{L}\)</span></p> <p>The <em>larger</em> the negative sample size <span class=arithmatex>\(N\)</span>, the <em>tighter</em> the bound.</p> <p>So we use <span class=arithmatex>\(N-1\)</span> negative samples.</p> <h4 id=instance-contrastive-learning>Instance Contrastive Learning<a class=headerlink href=#instance-contrastive-learning title="Permanent link">¶</a></h4> <h5 id=simclr><a href=https://arxiv.org/pdf/2002.05709.pdf>SimCLR</a><a class=headerlink href=#simclr title="Permanent link">¶</a></h5> <p>Use a projection function <span class=arithmatex>\(g(\cdot)\)</span> to project features to a space where contrastive learning is applied.</p> <p>The extra projection contributes a lot to the final performance.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_frame.jpg><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_frame.jpg></a></p> <p><strong>Score Function:</strong> Cos similarity <span class=arithmatex>\(s(u,v)=\frac{u^Tv}{||u||||v||}\\\)</span>.</p> <p><strong>Positive Pair:</strong> Pair of augmented data.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_algo.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_algo.png></a></p> <h5 id=momentum-contrastive-learning-moco><a href=https://arxiv.org/pdf/1911.05722.pdf>Momentum Contrastive Learning (MoCo)</a><a class=headerlink href=#momentum-contrastive-learning-moco title="Permanent link">¶</a></h5> <p>There are mainly <span class=arithmatex>\(3\)</span> training strategy in contrastive learning:</p> <ol> <li><em>end-to-end:</em> Keys are updated together with queries, e.g. <strong><em>SimCLR</em></strong>.</li> </ol> <p><strong>(limited by GPU size)</strong></p> <ol start=2> <li><em>memory bank:</em> Store last-time keys for sampling.</li> </ol> <p><strong>(inconsistency between <span class=arithmatex>\(q\)</span> and <span class=arithmatex>\(k\)</span>)</strong></p> <ol start=3> <li><em><strong>MoCo</strong>:</em> Use momentum methods to encode keys.</li> </ol> <p><strong>(combination of <em>end-to-end</em> &amp; <em>memory bank</em>)</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_cate.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_cate.png></a></p> <p><strong>Key differences to SimCLR:</strong></p> <ol> <li>Keep a running <strong>queue</strong> of keys (negative samples).</li> <li>Compute gradients and update the encoder <strong>only through the queries</strong>.</li> <li>Decouple min-batch size with the number of keys: can support <strong>a large number of negative samples</strong>.</li> <li>The key encoder is <strong>slowly progressing</strong> through the momentum update rules:</li> </ol> <p><span class=arithmatex>\(\theta_k\leftarrow m\theta_k+(1-m)\theta_q\)</span></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_algo.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_algo.png></a></p> <h4 id=sequence-contrastive-learning>Sequence Contrastive Learning<a class=headerlink href=#sequence-contrastive-learning title="Permanent link">¶</a></h4> <h5 id=contrastive-predictive-coding-cpc>Contrastive Predictive Coding (CPC)<a class=headerlink href=#contrastive-predictive-coding-cpc title="Permanent link">¶</a></h5> <p><strong>Contrastive:</strong> Contrast between “right” and “wrong” sequences using contrastive learning.</p> <p><strong>Predictive:</strong> The model has to <em>predict</em> future patterns given the current context.</p> <p><strong>Coding:</strong> The model learns useful <em>feature vectors</em>, or “code”, for downstream tasks, similar to other self-supervised methods.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_cpc.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_cpc.png></a></p> <h4 id=other-examples-frontier>Other Examples (Frontier)<a class=headerlink href=#other-examples-frontier title="Permanent link">¶</a></h4> <h5 id=contrastive-language-image-pre-training-clip>Contrastive Language Image Pre-training (CLIP)<a class=headerlink href=#contrastive-language-image-pre-training-clip title="Permanent link">¶</a></h5> <p>Contrastive learning between image and natural language sentences.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_clip.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_clip.png></a></p> <h2 id=15-low-level-vision>15 - Low-Level Vision<a class=headerlink href=#15-low-level-vision title="Permanent link">¶</a></h2> <p>Pass...</p> <h2 id=16-3d-vision>16 - 3D Vision<a class=headerlink href=#16-3d-vision title="Permanent link">¶</a></h2> <h3 id=representation>Representation<a class=headerlink href=#representation title="Permanent link">¶</a></h3> <h4 id=explicit-vs-implicit>Explicit vs Implicit<a class=headerlink href=#explicit-vs-implicit title="Permanent link">¶</a></h4> <p><strong>Explicit:</strong> Easy to sample examples, hard to do inside/outside check.</p> <p><strong>Implicit:</strong> Hard to sample examples, easy to do inside/outside check.</p> <table> <thead> <tr> <th></th> <th>Non-parametric</th> <th>Parametric</th> </tr> </thead> <tbody> <tr> <td><strong>Explicit</strong></td> <td>Points.<br>Meshes.</td> <td>Splines.<br>Subdivision Surfaces.</td> </tr> <tr> <td><strong>Implicit</strong></td> <td>Level Sets.<br>Voxels.</td> <td>Algebraic Surfaces.<br>Constructive Solid Geometry.</td> </tr> </tbody> </table> <h4 id=point-clouds>Point Clouds<a class=headerlink href=#point-clouds title="Permanent link">¶</a></h4> <p>The simplest representation.</p> <p>Collection of <span class=arithmatex>\((x,y,z)\)</span> coordinates.</p> <p><strong>Cons:</strong></p> <ol> <li>Difficult to draw in under-sampled regions.</li> <li>No simplification or subdivision.</li> <li>No direction smooth rendering.</li> <li>No topological information.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_point_clouds.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_point_clouds.png></a></p> <h4 id=polygonal-meshes>Polygonal Meshes<a class=headerlink href=#polygonal-meshes title="Permanent link">¶</a></h4> <p>Collection of vertices <span class=arithmatex>\(v\)</span> and edges <span class=arithmatex>\(e\)</span>.</p> <p><strong>Pros:</strong></p> <ol> <li>Can apply downsampling or upsampling on meshes.</li> <li>Error decreases by <span class=arithmatex>\(O(n^2)\)</span> while meshes increase by <span class=arithmatex>\(O(n)\)</span>.</li> <li>Can approximate arbitrary topology.</li> <li>Efficient rendering.</li> </ol> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_poly.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_poly.png></a></p> <h4 id=splines>Splines<a class=headerlink href=#splines title="Permanent link">¶</a></h4> <p>Use specific functions to approximate the surface. (e.g. Bézier Curves)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_bezier.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_bezier.png></a></p> <h4 id=algebraic-surfaces>Algebraic Surfaces<a class=headerlink href=#algebraic-surfaces title="Permanent link">¶</a></h4> <p>Use specific functions to represent the surface.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_algebra.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_algebra.png></a></p> <h4 id=constructive-solid-geometry>Constructive Solid Geometry<a class=headerlink href=#constructive-solid-geometry title="Permanent link">¶</a></h4> <p>Combine implicit geometry with Boolean operations.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_boolean.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_boolean.png></a></p> <h4 id=level-sets>Level Sets<a class=headerlink href=#level-sets title="Permanent link">¶</a></h4> <p>Store a grim of values to approximate the function.</p> <p>Surface is found where interpolated value equals to <span class=arithmatex>\(0\)</span>.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_level_set.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_level_set.png></a></p> <h4 id=voxels>Voxels<a class=headerlink href=#voxels title="Permanent link">¶</a></h4> <p>Binary thresholding the volumetric grid.</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=80% href=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_binary.png><img alt src=https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_binary.png></a></p> <h3 id=ai-3d>AI + 3D<a class=headerlink href=#ai-3d title="Permanent link">¶</a></h3> <p>Pass...</p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime">February 17, 2025 14:00:57</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime">February 17, 2025 14:00:57</span> </span> </aside> <div id=__comments></div> <!-- <h3>颜色主题调整</h3>
    <div class="tx-switch">
    <button class="button1" data-md-color-primary="red" style="background-color:red">red</button>
    <button class="button1" data-md-color-primary="pink" style="background-color:pink;color:black">pink</button>
    <button class="button1" data-md-color-primary="purple" style="background-color:purple">purple</button>
    <button class="button1" data-md-color-primary="indigo" style="background-color:indigo">indigo</button>
    <button class="button1" data-md-color-primary="blue" style="background-color:blue">blue</button>
    <button class="button1" data-md-color-primary="cyan" style="background-color:cyan;color:black">cyan</button>
    <button class="button1" data-md-color-primary="teal" style="background-color:teal">teal</button>
    <button class="button1" data-md-color-primary="green" style="background-color:green">green</button>
    <button class="button1" data-md-color-primary="lime" style="background-color:lime;color:black">lime</button>
    <button class="button1" data-md-color-primary="orange" style="background-color:orange;color:black">orange</button>
    <button class="button1" data-md-color-primary="brown" style="background-color:brown;border-radius=3px">brown</button>
    <button class="button1" data-md-color-primary="grey" style="background-color:grey">grey</button>
    <button class="button1" data-md-color-primary="black" style="background-color:black">black</button>
    <button class="button1" data-md-color-primary="white" style="background-color:white;color:black">white</button>
    </div> --> <!-- Giscus --> <script async crossorigin=anonymous data-category=General data-category-id=DIC_kwDONBPLqs4Cja0c data-emit-metadata=0 data-input-position=bottom data-lang=zh-CN data-mapping=pathname data-reactions-enabled=1 data-repo=WncFht/notes data-repo-id=R_kgDONBPLqg data-strict=0 data-theme=preferred_color_scheme src=https://giscus.app/client.js>
</script> <script>
    var buttons = document.querySelectorAll("button[data-md-color-primary]")
    buttons.forEach(function(button) {
            button.addEventListener("click", function() {
            var attr = this.getAttribute("data-md-color-primary")
            document.body.setAttribute("data-md-color-primary", attr)
            localStorage.setItem("data-md-color-primary",attr);
            })
    })
    </script> <!-- Synchronize Giscus theme with palette --> <script>
    var giscus = document.querySelector("script[src*=giscus]")
</script> <script>
    var giscus = document.querySelector("script[src*=giscus]")
    
    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate" ? "dark" : "light"
        giscus.setAttribute("data-theme", theme) 
    }
    
    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
            var theme = palette.color.scheme === "slate" ? "dark" : "light"
    
            /* Instruct Giscus to change theme */
            var frame = document.querySelector(".giscus-frame")
            frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
            )
        }
        })
    })
    </script> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2024 - Present <a href=https://github.com/zlflly/ rel=noopener target=_blank>zlflly</a> </div> Powered by <a href=https://www.mkdocs.org/ rel=noopener target=_blank> MkDocs </a> with theme <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material </a> modified by <a href=https://github.com/WncFht rel=noopener target=_blank> WncFht </a> </div> <div class=md-social> <a class=md-social__link href=https://github.com/zlflly rel=noopener target=_blank title=GitHub> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a> <a class=md-social__link href="https://space.bilibili.com/3546703389002196?spm_id_from=333.1387.0.0" rel=noopener target=_blank title=Bilibili> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0q13.2 0 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0q13.2 0 21.9 8.603c5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1m-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8s9.7-14.3 10.1-23.5zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5s-17.5-3.2-23.6-9.5-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2s13.2-9.6 23.3-10c9.2.4 17 3.7 23.3 10m191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2s-14 9.5-23.6 9.5-17.4-3.2-23.6-9.5c-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2s14.1-9.6 23.3-10c9.2.4 17 3.7 23.3 10"></path></svg> </a> <a class=md-social__link href=https://www.zhihu.com/people/ha-ha-ha-ha-ha-ha-38-72 rel=noopener target=_blank title=Zhihu> <svg viewbox="0 0 640 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"></path></svg> </a> <a class=md-social__link href=assets/images/qq.jpg rel=noopener target=_blank title=QQ> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433.754 420.445c-11.526 1.393-44.86-52.741-44.86-52.741 0 31.345-16.136 72.247-51.051 101.786 16.842 5.192 54.843 19.167 45.803 34.421-7.316 12.343-125.51 7.881-159.632 4.037-34.122 3.844-152.316 8.306-159.632-4.037-9.045-15.25 28.918-29.214 45.783-34.415-34.92-29.539-51.059-70.445-51.059-101.792 0 0-33.334 54.134-44.859 52.741-5.37-.65-12.424-29.644 9.347-99.704 10.261-33.024 21.995-60.478 40.144-105.779C60.683 98.063 108.982.006 224 0c113.737.006 163.156 96.133 160.264 214.963 18.118 45.223 29.912 72.85 40.144 105.778 21.768 70.06 14.716 99.053 9.346 99.704"></path></svg> </a> <a class=md-social__link href=assets/images/WeChat.jpg rel=noopener target=_blank title=WeChat> <svg viewbox="0 0 576 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6"></path></svg> </a> <a class=md-social__link href=mailto:2158363941@qq.com rel=noopener target=_blank title=Email> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.code.select", "content.action.view", "content.footnote.tooltips", "content.tabs.link", "header.autohide", "navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.f1b6f286.min.js></script> <script src=../../../js/katex.min.js></script> <script src=../../../js/toc.min.js></script> <script src=../../../js/mathjax.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.tonycrane.cc/utils/katex.min.js></script> <script src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml-full.js></script> <script src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg-full.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>